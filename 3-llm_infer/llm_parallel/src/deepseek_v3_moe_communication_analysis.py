"""
DeepSeek V3 MoE é€šä¿¡é‡åˆ†æä¸ All2All å¯è§†åŒ–

åŸºäºDeepSeek V3/R1ä¸“å®¶å¹¶è¡Œç­–ç•¥çš„è¯¦ç»†é€šä¿¡åˆ†æï¼š
- Prefill: è·¯ç”±ä¸“å®¶ EP32ã€MLA å’Œå…±äº«ä¸“å®¶ DP32ï¼Œ4èŠ‚ç‚¹éƒ¨ç½²å•å…ƒ
- Decode: è·¯ç”±ä¸“å®¶ EP144ã€MLA å’Œå…±äº«ä¸“å®¶ DP144ï¼Œ18èŠ‚ç‚¹éƒ¨ç½²å•å…ƒ

ä½œè€…: åŸºäºDeepSeek V3è®ºæ–‡å’Œå®é™…éƒ¨ç½²ç­–ç•¥
æ—¥æœŸ: 2024
"""

import torch
import torch.distributed as dist
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sns
from typing import Dict, List, Tuple, Optional
import math
from dataclasses import dataclass
import time

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

@dataclass
class EPConfig:
    """ä¸“å®¶å¹¶è¡Œé…ç½®"""
    # åŸºç¡€é…ç½®
    total_experts: int = 256  # æ€»ä¸“å®¶æ•°
    experts_per_token: int = 8  # æ¯ä¸ªtokené€‰æ‹©çš„ä¸“å®¶æ•°
    hidden_size: int = 7168  # éšè—å±‚ç»´åº¦
    moe_intermediate_size: int = 1407  # MoEä¸­é—´å±‚å¤§å°
    
    # Prefillé˜¶æ®µé…ç½®
    prefill_ep_size: int = 32  # è·¯ç”±ä¸“å®¶å¹¶è¡Œåº¦
    prefill_dp_size: int = 32  # å…±äº«ä¸“å®¶æ•°æ®å¹¶è¡Œåº¦
    prefill_nodes: int = 4  # èŠ‚ç‚¹æ•°
    prefill_gpus_per_node: int = 8  # æ¯èŠ‚ç‚¹GPUæ•°
    prefill_experts_per_gpu: int = 9  # æ¯GPUè·¯ç”±ä¸“å®¶æ•°
    prefill_shared_experts_per_gpu: int = 1  # æ¯GPUå…±äº«ä¸“å®¶æ•°
    
    # Decodeé˜¶æ®µé…ç½®
    decode_ep_size: int = 144  # è·¯ç”±ä¸“å®¶å¹¶è¡Œåº¦
    decode_dp_size: int = 144  # å…±äº«ä¸“å®¶æ•°æ®å¹¶è¡Œåº¦
    decode_nodes: int = 18  # èŠ‚ç‚¹æ•°
    decode_gpus_per_node: int = 8  # æ¯èŠ‚ç‚¹GPUæ•°
    decode_experts_per_gpu: int = 2  # æ¯GPUè·¯ç”±ä¸“å®¶æ•°
    decode_shared_experts_per_gpu: int = 1  # æ¯GPUå…±äº«ä¸“å®¶æ•°

class MoECommunicationAnalyzer:
    """MoEé€šä¿¡é‡åˆ†æå™¨"""
    
    def __init__(self, config: EPConfig):
        self.config = config
        self.nvlink_bandwidth = 400.0  # GB/s (H800)
        self.ib_bandwidth = 50.0  # GB/s (InfiniBand)
        
    def calculate_token_distribution(self, batch_size: int, seq_len: int, 
                                   experts_per_token: int = 8) -> Dict:
        """è®¡ç®—tokenåˆ°ä¸“å®¶çš„åˆ†å¸ƒ"""
        total_tokens = batch_size * seq_len
        total_expert_assignments = total_tokens * experts_per_token
        
        # ç†æƒ³è´Ÿè½½å‡è¡¡ï¼šæ¯ä¸ªä¸“å®¶è·å¾—ç›¸åŒæ•°é‡çš„token
        tokens_per_expert = total_expert_assignments / self.config.total_experts
        
        return {
            'total_tokens': total_tokens,
            'total_expert_assignments': total_expert_assignments,
            'tokens_per_expert': tokens_per_expert,
            'load_balance_ratio': 1.0  # ç†æƒ³æƒ…å†µ
        }
    
    def analyze_prefill_communication(self, batch_size: int, seq_len: int) -> Dict:
        """åˆ†æPrefillé˜¶æ®µçš„é€šä¿¡é‡"""
        print("=" * 80)
        print("ğŸ“Š Prefillé˜¶æ®µé€šä¿¡é‡åˆ†æ")
        print("=" * 80)
        
        # åŸºç¡€å‚æ•°
        ep_size = self.config.prefill_ep_size
        nodes = self.config.prefill_nodes
        gpus_per_node = self.config.prefill_gpus_per_node
        experts_per_gpu = self.config.prefill_experts_per_gpu
        
        # Tokenåˆ†å¸ƒ
        token_dist = self.calculate_token_distribution(batch_size, seq_len)
        tokens_per_expert = token_dist['tokens_per_expert']
        
        # æ¯ä¸ªGPUçš„ä¸“å®¶åˆ†å¸ƒ
        experts_per_gpu = self.config.total_experts // ep_size
        tokens_per_gpu = tokens_per_expert * experts_per_gpu
        
        # é€šä¿¡é‡è®¡ç®—
        hidden_size = self.config.hidden_size
        dtype_size = 2  # BF16 = 2 bytes
        
        # All2Allé€šä¿¡é‡
        # 1. å‘é€tokensåˆ°å¯¹åº”ä¸“å®¶
        send_volume = tokens_per_gpu * hidden_size * dtype_size  # bytes
        send_volume_gb = send_volume / (1024**3)
        
        # 2. æ¥æ”¶å¤„ç†åçš„tokens
        receive_volume = send_volume
        receive_volume_gb = receive_volume / (1024**3)
        
        # 3. æ€»é€šä¿¡é‡
        total_volume_gb = send_volume_gb + receive_volume_gb
        
        # é€šä¿¡æ¨¡å¼åˆ†æ
        # èŠ‚ç‚¹å†…ï¼šNVLink (400 GB/s)
        # èŠ‚ç‚¹é—´ï¼šInfiniBand (50 GB/s)
        intra_node_comm = send_volume_gb * (gpus_per_node - 1) / gpus_per_node
        inter_node_comm = send_volume_gb * (ep_size - gpus_per_node) / ep_size
        
        # ç†è®ºé€šä¿¡æ—¶é—´
        intra_node_time = intra_node_comm / self.nvlink_bandwidth
        inter_node_time = inter_node_comm / self.ib_bandwidth
        total_comm_time = max(intra_node_time, inter_node_time)
        
        result = {
            'phase': 'prefill',
            'ep_size': ep_size,
            'nodes': nodes,
            'gpus_per_node': gpus_per_node,
            'experts_per_gpu': experts_per_gpu,
            'tokens_per_gpu': tokens_per_gpu,
            'send_volume_gb': send_volume_gb,
            'receive_volume_gb': receive_volume_gb,
            'total_volume_gb': total_volume_gb,
            'intra_node_comm_gb': intra_node_comm,
            'inter_node_comm_gb': inter_node_comm,
            'intra_node_time_ms': intra_node_time * 1000,
            'inter_node_time_ms': inter_node_time * 1000,
            'total_comm_time_ms': total_comm_time * 1000,
            'token_distribution': token_dist
        }
        
        print(f"ğŸ”§ é…ç½®ä¿¡æ¯:")
        print(f"   - ä¸“å®¶å¹¶è¡Œåº¦: {ep_size}")
        print(f"   - èŠ‚ç‚¹æ•°: {nodes}")
        print(f"   - æ¯èŠ‚ç‚¹GPUæ•°: {gpus_per_node}")
        print(f"   - æ¯GPUä¸“å®¶æ•°: {experts_per_gpu}")
        
        print(f"\nğŸ“ˆ é€šä¿¡é‡åˆ†æ:")
        print(f"   - æ¯GPUå‘é€é‡: {send_volume_gb:.2f} GB")
        print(f"   - æ¯GPUæ¥æ”¶é‡: {receive_volume_gb:.2f} GB")
        print(f"   - æ€»é€šä¿¡é‡: {total_volume_gb:.2f} GB")
        print(f"   - èŠ‚ç‚¹å†…é€šä¿¡: {intra_node_comm:.2f} GB")
        print(f"   - èŠ‚ç‚¹é—´é€šä¿¡: {inter_node_comm:.2f} GB")
        
        print(f"\nâ±ï¸ é€šä¿¡æ—¶é—´åˆ†æ:")
        print(f"   - èŠ‚ç‚¹å†…é€šä¿¡æ—¶é—´: {intra_node_time*1000:.2f} ms")
        print(f"   - èŠ‚ç‚¹é—´é€šä¿¡æ—¶é—´: {inter_node_time*1000:.2f} ms")
        print(f"   - æ€»é€šä¿¡æ—¶é—´: {total_comm_time*1000:.2f} ms")
        
        return result
    
    def analyze_decode_communication(self, batch_size: int, seq_len: int = 1) -> Dict:
        """åˆ†æDecodeé˜¶æ®µçš„é€šä¿¡é‡"""
        print("\n" + "=" * 80)
        print("ğŸ“Š Decodeé˜¶æ®µé€šä¿¡é‡åˆ†æ")
        print("=" * 80)
        
        # åŸºç¡€å‚æ•°
        ep_size = self.config.decode_ep_size
        nodes = self.config.decode_nodes
        gpus_per_node = self.config.decode_gpus_per_node
        experts_per_gpu = self.config.decode_experts_per_gpu
        
        # Tokenåˆ†å¸ƒ (decodeé˜¶æ®µseq_len=1)
        token_dist = self.calculate_token_distribution(batch_size, seq_len)
        tokens_per_expert = token_dist['tokens_per_expert']
        
        # æ¯ä¸ªGPUçš„ä¸“å®¶åˆ†å¸ƒ
        experts_per_gpu = self.config.total_experts // ep_size
        tokens_per_gpu = tokens_per_expert * experts_per_gpu
        
        # é€šä¿¡é‡è®¡ç®—
        hidden_size = self.config.hidden_size
        dtype_size = 2  # BF16 = 2 bytes
        
        # All2Allé€šä¿¡é‡
        send_volume = tokens_per_gpu * hidden_size * dtype_size
        send_volume_gb = send_volume / (1024**3)
        receive_volume_gb = send_volume_gb
        total_volume_gb = send_volume_gb + receive_volume_gb
        
        # é€šä¿¡æ¨¡å¼åˆ†æ
        intra_node_comm = send_volume_gb * (gpus_per_node - 1) / gpus_per_node
        inter_node_comm = send_volume_gb * (ep_size - gpus_per_node) / ep_size
        
        # ç†è®ºé€šä¿¡æ—¶é—´
        intra_node_time = intra_node_comm / self.nvlink_bandwidth
        inter_node_time = inter_node_comm / self.ib_bandwidth
        total_comm_time = max(intra_node_time, inter_node_time)
        
        result = {
            'phase': 'decode',
            'ep_size': ep_size,
            'nodes': nodes,
            'gpus_per_node': gpus_per_node,
            'experts_per_gpu': experts_per_gpu,
            'tokens_per_gpu': tokens_per_gpu,
            'send_volume_gb': send_volume_gb,
            'receive_volume_gb': receive_volume_gb,
            'total_volume_gb': total_volume_gb,
            'intra_node_comm_gb': intra_node_comm,
            'inter_node_comm_gb': inter_node_comm,
            'intra_node_time_ms': intra_node_time * 1000,
            'inter_node_time_ms': inter_node_time * 1000,
            'total_comm_time_ms': total_comm_time * 1000,
            'token_distribution': token_dist
        }
        
        print(f"ğŸ”§ é…ç½®ä¿¡æ¯:")
        print(f"   - ä¸“å®¶å¹¶è¡Œåº¦: {ep_size}")
        print(f"   - èŠ‚ç‚¹æ•°: {nodes}")
        print(f"   - æ¯èŠ‚ç‚¹GPUæ•°: {gpus_per_node}")
        print(f"   - æ¯GPUä¸“å®¶æ•°: {experts_per_gpu}")
        
        print(f"\nğŸ“ˆ é€šä¿¡é‡åˆ†æ:")
        print(f"   - æ¯GPUå‘é€é‡: {send_volume_gb:.4f} GB")
        print(f"   - æ¯GPUæ¥æ”¶é‡: {receive_volume_gb:.4f} GB")
        print(f"   - æ€»é€šä¿¡é‡: {total_volume_gb:.4f} GB")
        print(f"   - èŠ‚ç‚¹å†…é€šä¿¡: {intra_node_comm:.4f} GB")
        print(f"   - èŠ‚ç‚¹é—´é€šä¿¡: {inter_node_comm:.4f} GB")
        
        print(f"\nâ±ï¸ é€šä¿¡æ—¶é—´åˆ†æ:")
        print(f"   - èŠ‚ç‚¹å†…é€šä¿¡æ—¶é—´: {intra_node_time*1000:.4f} ms")
        print(f"   - èŠ‚ç‚¹é—´é€šä¿¡æ—¶é—´: {inter_node_time*1000:.4f} ms")
        print(f"   - æ€»é€šä¿¡æ—¶é—´: {total_comm_time*1000:.4f} ms")
        
        return result

class All2AllVisualizer:
    """All2Allé€šä¿¡è¿‡ç¨‹å¯è§†åŒ–å™¨"""
    
    def __init__(self, config: EPConfig):
        self.config = config
        
    def visualize_prefill_all2all(self, batch_size: int, seq_len: int):
        """å¯è§†åŒ–Prefillé˜¶æ®µçš„All2Allè¿‡ç¨‹"""
        print("\n" + "=" * 80)
        print("ğŸ¨ Prefillé˜¶æ®µ All2All å¯è§†åŒ–")
        print("=" * 80)
        
        ep_size = self.config.prefill_ep_size
        nodes = self.config.prefill_nodes
        gpus_per_node = self.config.prefill_gpus_per_node
        
        # åˆ›å»ºå›¾å½¢
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 12))
        
        # 1. ä¸“å®¶åˆ†å¸ƒå›¾
        self._plot_expert_distribution(ax1, ep_size, nodes, gpus_per_node, "Prefill")
        
        # 2. All2Allé€šä¿¡æµç¨‹å›¾
        self._plot_all2all_communication(ax2, ep_size, nodes, gpus_per_node, "Prefill")
        
        plt.tight_layout()
        plt.savefig('prefill_all2all_visualization.png', dpi=300, bbox_inches='tight')
        plt.show()
        
    def visualize_decode_all2all(self, batch_size: int):
        """å¯è§†åŒ–Decodeé˜¶æ®µçš„All2Allè¿‡ç¨‹"""
        print("\n" + "=" * 80)
        print("ğŸ¨ Decodeé˜¶æ®µ All2All å¯è§†åŒ–")
        print("=" * 80)
        
        ep_size = self.config.decode_ep_size
        nodes = self.config.decode_nodes
        gpus_per_node = self.config.decode_gpus_per_node
        
        # åˆ›å»ºå›¾å½¢
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 12))
        
        # 1. ä¸“å®¶åˆ†å¸ƒå›¾
        self._plot_expert_distribution(ax1, ep_size, nodes, gpus_per_node, "Decode")
        
        # 2. All2Allé€šä¿¡æµç¨‹å›¾
        self._plot_all2all_communication(ax2, ep_size, nodes, gpus_per_node, "Decode")
        
        plt.tight_layout()
        plt.savefig('decode_all2all_visualization.png', dpi=300, bbox_inches='tight')
        plt.show()
        
    def _plot_expert_distribution(self, ax, ep_size: int, nodes: int, gpus_per_node: int, phase: str):
        """ç»˜åˆ¶ä¸“å®¶åˆ†å¸ƒå›¾"""
        total_gpus = ep_size
        experts_per_gpu = self.config.total_experts // ep_size
        
        # åˆ›å»ºGPUç½‘æ ¼
        gpu_positions = []
        for node in range(nodes):
            for gpu in range(gpus_per_node):
                if node * gpus_per_node + gpu < total_gpus:
                    gpu_positions.append((node, gpu))
        
        # ç»˜åˆ¶GPUå’Œä¸“å®¶
        colors = plt.cm.Set3(np.linspace(0, 1, experts_per_gpu))
        
        for i, (node, gpu) in enumerate(gpu_positions):
            # GPUä½ç½®
            x = node * 3
            y = gpu * 2
            
            # ç»˜åˆ¶GPUæ¡†
            rect = patches.Rectangle((x-0.4, y-0.4), 0.8, 0.8, 
                                   linewidth=2, edgecolor='black', 
                                   facecolor='lightblue', alpha=0.7)
            ax.add_patch(rect)
            ax.text(x, y, f'GPU\n{node}-{gpu}', ha='center', va='center', fontsize=8)
            
            # ç»˜åˆ¶ä¸“å®¶
            for j in range(experts_per_gpu):
                expert_x = x - 0.3 + j * 0.15
                expert_y = y + 0.6
                circle = patches.Circle((expert_x, expert_y), 0.05, 
                                      facecolor=colors[j], edgecolor='black', linewidth=1)
                ax.add_patch(circle)
                ax.text(expert_x, expert_y, f'E{i*experts_per_gpu+j}', 
                       ha='center', va='center', fontsize=6)
        
        ax.set_xlim(-1, nodes * 3 - 1)
        ax.set_ylim(-1, gpus_per_node * 2 - 1)
        ax.set_xlabel('èŠ‚ç‚¹')
        ax.set_ylabel('GPU')
        ax.set_title(f'{phase}é˜¶æ®µä¸“å®¶åˆ†å¸ƒ (EP{ep_size}, {nodes}èŠ‚ç‚¹)')
        ax.grid(True, alpha=0.3)
        
    def _plot_all2all_communication(self, ax, ep_size: int, nodes: int, gpus_per_node: int, phase: str):
        """ç»˜åˆ¶All2Allé€šä¿¡æµç¨‹å›¾"""
        total_gpus = ep_size
        
        # åˆ›å»ºGPUä½ç½®
        gpu_positions = []
        for node in range(nodes):
            for gpu in range(gpus_per_node):
                if node * gpus_per_node + gpu < total_gpus:
                    gpu_positions.append((node, gpu))
        
        # ç»˜åˆ¶GPU
        for i, (node, gpu) in enumerate(gpu_positions):
            x = i * 2
            y = 0
            
            # GPUæ¡†
            rect = patches.Rectangle((x-0.8, y-0.4), 1.6, 0.8, 
                                   linewidth=2, edgecolor='black', 
                                   facecolor='lightgreen', alpha=0.7)
            ax.add_patch(rect)
            ax.text(x, y, f'GPU{node}-{gpu}', ha='center', va='center', fontsize=8)
        
        # ç»˜åˆ¶All2Allé€šä¿¡
        for i in range(total_gpus):
            for j in range(total_gpus):
                if i != j:
                    x1 = i * 2
                    x2 = j * 2
                    y1 = 0.6
                    y2 = 0.6
                    
                    # èŠ‚ç‚¹å†…é€šä¿¡ç”¨å®çº¿ï¼ŒèŠ‚ç‚¹é—´é€šä¿¡ç”¨è™šçº¿
                    node_i = i // gpus_per_node
                    node_j = j // gpus_per_node
                    
                    if node_i == node_j:
                        # èŠ‚ç‚¹å†…é€šä¿¡ (NVLink)
                        ax.plot([x1, x2], [y1, y2], 'b-', alpha=0.3, linewidth=1)
                    else:
                        # èŠ‚ç‚¹é—´é€šä¿¡ (InfiniBand)
                        ax.plot([x1, x2], [y1, y2], 'r--', alpha=0.3, linewidth=1)
        
        ax.set_xlim(-1, total_gpus * 2 - 1)
        ax.set_ylim(-1, 2)
        ax.set_xlabel('GPU ID')
        ax.set_ylabel('é€šä¿¡å±‚')
        ax.set_title(f'{phase}é˜¶æ®µ All2All é€šä¿¡æ¨¡å¼')
        
        # æ·»åŠ å›¾ä¾‹
        ax.plot([], [], 'b-', label='èŠ‚ç‚¹å†…é€šä¿¡ (NVLink)', linewidth=2)
        ax.plot([], [], 'r--', label='èŠ‚ç‚¹é—´é€šä¿¡ (InfiniBand)', linewidth=2)
        ax.legend(loc='upper right')

def simulate_moe_communication():
    """æ¨¡æ‹ŸMoEé€šä¿¡è¿‡ç¨‹"""
    print("ğŸš€ DeepSeek V3 MoE é€šä¿¡é‡åˆ†æä¸å¯è§†åŒ–")
    print("=" * 80)
    
    # åˆ›å»ºé…ç½®
    config = EPConfig()
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = MoECommunicationAnalyzer(config)
    visualizer = All2AllVisualizer(config)
    
    # åˆ†æPrefillé˜¶æ®µ
    batch_size = 1024
    seq_len = 2048
    prefill_result = analyzer.analyze_prefill_communication(batch_size, seq_len)
    
    # åˆ†æDecodeé˜¶æ®µ
    decode_result = analyzer.analyze_decode_communication(batch_size, seq_len=1)
    
    # å¯è§†åŒ–
    visualizer.visualize_prefill_all2all(batch_size, seq_len)
    visualizer.visualize_decode_all2all(batch_size)
    
    # å¯¹æ¯”åˆ†æ
    print("\n" + "=" * 80)
    print("ğŸ“Š Prefill vs Decode å¯¹æ¯”åˆ†æ")
    print("=" * 80)
    
    print(f"ğŸ”§ å¹¶è¡Œåº¦å¯¹æ¯”:")
    print(f"   - Prefill EP: {prefill_result['ep_size']} vs Decode EP: {decode_result['ep_size']}")
    print(f"   - Prefill èŠ‚ç‚¹: {prefill_result['nodes']} vs Decode èŠ‚ç‚¹: {decode_result['nodes']}")
    print(f"   - Prefill æ¯GPUä¸“å®¶: {prefill_result['experts_per_gpu']} vs Decode æ¯GPUä¸“å®¶: {decode_result['experts_per_gpu']}")
    
    print(f"\nğŸ“ˆ é€šä¿¡é‡å¯¹æ¯”:")
    print(f"   - Prefill æ€»é€šä¿¡é‡: {prefill_result['total_volume_gb']:.2f} GB")
    print(f"   - Decode æ€»é€šä¿¡é‡: {decode_result['total_volume_gb']:.4f} GB")
    print(f"   - é€šä¿¡é‡æ¯”ä¾‹: {prefill_result['total_volume_gb']/decode_result['total_volume_gb']:.0f}:1")
    
    print(f"\nâ±ï¸ é€šä¿¡æ—¶é—´å¯¹æ¯”:")
    print(f"   - Prefill é€šä¿¡æ—¶é—´: {prefill_result['total_comm_time_ms']:.2f} ms")
    print(f"   - Decode é€šä¿¡æ—¶é—´: {decode_result['total_comm_time_ms']:.4f} ms")
    print(f"   - æ—¶é—´æ¯”ä¾‹: {prefill_result['total_comm_time_ms']/decode_result['total_comm_time_ms']:.0f}:1")
    
    return prefill_result, decode_result

if __name__ == "__main__":
    prefill_result, decode_result = simulate_moe_communication() 