import torch
import torch.nn as nn
from typing import Optional, Tuple
import numpy as np

# å®šä¹‰è£…é¥°å™¨ï¼ˆå¦‚æœæ²¡æœ‰åŠ¨æ€æ›´æ–°éœ€æ±‚ï¼Œå¯ä»¥ä½¿ç”¨ç©ºè£…é¥°å™¨ï¼‰
def dynamic_rope_update(func):
    return func

# å®šä¹‰RoPEåˆå§‹åŒ–å‡½æ•°å­—å…¸
ROPE_INIT_FUNCTIONS = {
    "default": lambda config, device: default_rope_init(config, device),
    # æ·»åŠ å…¶ä»–åˆå§‹åŒ–ç±»å‹...
}

def default_rope_init(config, device) -> Tuple[torch.Tensor, float]:
    """é»˜è®¤RoPEåˆå§‹åŒ–å‡½æ•°"""
    # è®¡ç®—æ¯ä¸ªå¤´çš„ç»´åº¦
    head_dim = config.hidden_size // config.num_attention_heads
    
    # è®¡ç®—åŸºç¡€é¢‘ç‡
    base = getattr(config, "rope_theta", 10000.0)
    inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2, device=device).float() / head_dim))
    
    # é»˜è®¤ç¼©æ”¾å› å­ä¸º1.0
    attention_scaling = 1.0
    return inv_freq, attention_scaling

# ç®€åŒ–çš„é…ç½®ç±»
class Qwen3Config:
    def __init__(self, rope_scaling=None, max_position_embeddings=4096, 
                 hidden_size=4096, num_attention_heads=32, rope_theta=10000.0):
        self.rope_scaling = rope_scaling
        self.max_position_embeddings = max_position_embeddings
        self.hidden_size = hidden_size
        self.num_attention_heads = num_attention_heads
        self.rope_theta = rope_theta

class Qwen3RotaryEmbedding(nn.Module):
    """
    ç”¨äº Qwen-3 ç³»åˆ—æ¨¡å‹çš„æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰å¼ é‡æ„é€ å™¨ã€‚
    è¯¥æ¨¡å—åªè´Ÿè´£è®¡ç®— cos/sin ä¸¤å¼ æŸ¥è¡¨å¼ é‡ï¼Œä¾›åç»­ q,k å¼ é‡åšæ—‹è½¬ã€‚
    """
    def __init__(self, config: Qwen3Config, device: Optional[str] = None):
        super().__init__()
        
        # ---- â‘  è§£æ RoPE å­ç±»å‹ ----
        if hasattr(config, "rope_scaling") and config.rope_scaling is not None:
            self.rope_type = config.rope_scaling.get(
                "rope_type",  # æ–°å­—æ®µ
                config.rope_scaling.get("type", "default")  # æ—§å­—æ®µå…œåº•
            )
        else:
            self.rope_type = "default"  # è‹¥æ²¡é…ï¼Œåˆ™èµ°é»˜è®¤å®ç°

        # ---- â‘¡ è®°å½•æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆç¼“å­˜å¤§å°ï¼‰----
        self.max_seq_len_cached = config.max_position_embeddings
        self.original_max_seq_len = config.max_position_embeddings

        # ---- â‘¢ ä¿å­˜ config å¹¶é€‰æ‹©åˆå§‹åŒ–å‡½æ•° ----
        self.config = config
        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]

        # ---- â‘£ ç”Ÿæˆ inv_freq ä¸ç¼©æ”¾å› å­ ----
        #   inv_freq shape: (head_dim//2,)ï¼Œå†…å®¹ä¸º 1/Î¸^i
        #   attention_scaling: é’ˆå¯¹éƒ¨åˆ† RoPE å˜ä½“çš„é¢å¤–ç¼©æ”¾
        inv_freq, self.attention_scaling = self.rope_init_fn(config, device)

        # æ³¨å†Œä¸º buffer â†’ ä¿å­˜åˆ° state_dictï¼Œä½†ä¸ç®—æ¨¡å‹å‚æ•°
        self.register_buffer("inv_freq", inv_freq, persistent=False)
        self.original_inv_freq = self.inv_freq  # å†å­˜ä¸€ä»½å¤‡ä»½ï¼Œä¾¿äºåŠ¨æ€æ‰©å±•
        head_dim = config.hidden_size // config.num_attention_heads
        # æ‰“å°åˆå§‹åŒ–ä¿¡æ¯
        print(f"ğŸ”§ åˆå§‹åŒ– RoPE ç¼–ç å™¨: ç±»å‹={self.rope_type}, æœ€å¤§ä½ç½®={self.max_seq_len_cached}")
        print(f"  å¤´ç»´åº¦={head_dim}, é¢‘ç‡å‚æ•°å½¢çŠ¶={inv_freq.shape}, ç¼©æ”¾å› å­={self.attention_scaling}")

    # --- å‰å‘è®¡ç®— ---
    @torch.no_grad()          # ä¸éœ€è¦æ¢¯åº¦
    @dynamic_rope_update      # é«˜é˜¶è£…é¥°å™¨ï¼šæ”¯æŒåœ¨çº¿æ‰©å±• RoPE é•¿åº¦
    def forward(self, x: torch.Tensor, position_ids: torch.Tensor):
        """
        å‚æ•°
        ----
        x:            (bs, seq, hidden_size) åªæ˜¯ç”¨æ¥æ‹¿ dtype/device
        position_ids: (bs, seq)              æ¯ä¸ª token çš„ç»å¯¹ä½ç½®
        
        è¿”å›
        ----
        cos, sin: äºŒå¼ æŸ¥è¡¨å¼ é‡ï¼Œshape=(bs, seq, head_dim)
        """
        # 0. è®¾å¤‡å¤„ç†ï¼ˆç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Šï¼‰
        device = x.device
        
        # 1. å°† inv_freq å°ºå¯¸ [head_dim//2] æ‰©å±•åˆ° (bs, head_dim//2, 1)
        inv_freq_expanded = (self.inv_freq[None, :, None]  # [1, head_dim//2, 1]
                            .float()                      # ç¡®ä¿fp32ç²¾åº¦
                            .expand(position_ids.shape[0], -1, 1)  # [bs, head_dim//2, 1]
                            .to(device)) # ç¡®ä¿å¼ é‡ä½äºæ­£ç¡®çš„è®¡ç®—è®¾å¤‡ä¸Šï¼ˆCPUæˆ–GPUï¼‰
        print("ã€é¢‘ç‡å› å­ã€‘inv_freq shape:", inv_freq_expanded.shape)
        print("ã€æ‹“å±•åçš„é¢‘ç‡å› å­ã€‘inv_freq_expanded shape:", inv_freq_expanded.shape)

        # 2. å°† position_ids (bs, seq) æ‰©å±•åˆ° (bs, 1, seq)
        position_ids_expanded = position_ids[:, None, :].float()  # [bs, 1, seq]

        # 3. æŒ‡å®š autocast çš„è®¾å¤‡ç±»å‹ï¼ˆMPS ä¾‹å¤–éœ€é€€å› cpuï¼‰
        device_type = "cpu" if device.type == "mps" else device.type

        # 4. å¼ºåˆ¶ç¦ç”¨ autocast â†’ ç”¨ fp32 è®¡ç®—è§’åº¦ï¼Œé˜²æ­¢ç²¾åº¦æŸå¤±
        with torch.autocast(device_type=device_type, enabled=False):
            # çŸ©é˜µä¹˜æ³•: [bs, head_dim//2, 1] @ (bs, 1, seq) â†’ (bs, head_dim//2, seq)
            # ç»“æœ freqs å¼ é‡åŒ…å«äº†ç”¨äºåç»­sin/cosè®¡ç®—çš„è§’åº¦å€¼ã€‚
            freqs = torch.matmul(inv_freq_expanded, position_ids_expanded)
            
            # è½¬ç½®: (bs, head_dim//2, seq) â†’ (bs, seq, head_dim//2)
            freqs = freqs.transpose(1, 2)
            
            # æ‹¼æ¥å¶ã€å¥‡ç»´åº¦ â†’ (bs, seq, head_dim)
            emb = torch.cat((freqs, freqs), dim=-1)

            # å– cos / sinï¼ˆå†ä¹˜å¯é€‰ scalingï¼‰
            cos = emb.cos() * self.attention_scaling
            sin = emb.sin() * self.attention_scaling

        # 5. ä¸è¾“å…¥å¼ é‡ä¿æŒä¸€è‡´çš„ dtype è¿”å›
        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)

# ===== å¢å¼ºç‰ˆæµ‹è¯•å‡½æ•° =====
def test_rotary_embedding(visualize: bool = True):
    """å…¨é¢æµ‹è¯• RoPE å®ç°å¹¶è¾“å‡ºè¯¦ç»†åˆ†æ"""
    print("\n" + "="*60)
    print("ğŸ”¥ Qwen-3 RoPE æ—‹è½¬ä½ç½®ç¼–ç  å…¨é¢æµ‹è¯•")
    print("="*60)
    
    # é…ç½®å‚æ•°
    bs, seq, n_heads, head_dim = 2, 16, 32, 128
    hidden_size = n_heads * head_dim
    
    print(f"\nğŸ“‹ æµ‹è¯•é…ç½®:")
    print(f"  æ‰¹å¤§å° (bs) = {bs}")
    print(f"  åºåˆ—é•¿åº¦ (seq) = {seq}")
    print(f"  æ³¨æ„åŠ›å¤´æ•° (n_heads) = {n_heads}")
    print(f"  æ¯ä¸ªå¤´çš„ç»´åº¦ (head_dim) = {head_dim}")
    print(f"  æ€»éšè—å¤§å° (hidden_size) = {hidden_size}")
    
    # åˆ›å»ºé…ç½®å¯¹è±¡
    dummy_cfg = Qwen3Config(
        rope_scaling={"rope_type": "default"},
        max_position_embeddings=4096,
        hidden_size=hidden_size,
        num_attention_heads=n_heads,
        rope_theta=10000.0
    )

    # åˆ›å»ºRoPEæ¨¡å—
    print("\nğŸ›  åˆ›å»º RoPE ç¼–ç å™¨...")
    rot = Qwen3RotaryEmbedding(dummy_cfg, device="cpu")
    
    # æ‰“å°é¢‘ç‡å‚æ•°ä¿¡æ¯
    inv_freq = rot.inv_freq.cpu().numpy()
    print(f"\nğŸ“Š é¢‘ç‡å‚æ•°åˆ†æ (inv_freq):")
    print(f"  å½¢çŠ¶: {inv_freq.shape}")
    print(f"  æœ€å°å€¼: {inv_freq.min():.6f}")
    print(f"  æœ€å¤§å€¼: {inv_freq.max():.6f}")
    print(f"  å¹³å‡å€¼: {inv_freq.mean():.6f}")
    print(f"  å‰5ä¸ªå€¼: {inv_freq[:5].round(6)}")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    x = torch.randn(bs, seq, hidden_size)  # æ¨¡æ‹Ÿè¾“å…¥
    position_ids = torch.arange(seq).repeat(bs, 1)  # (bs, seq)
    
    print("\nâš¡ è®¡ç®— RoPE ç¼–ç ...")
    cos, sin = rot(x, position_ids)
    
    # éªŒè¯è¾“å‡º
    print("\nâœ… è¾“å‡ºéªŒè¯:")
    print(f"  cos å½¢çŠ¶: {cos.shape} â†’ (æ‰¹å¤§å°, åºåˆ—é•¿åº¦, head_dim)")
    print(f"  sin å½¢çŠ¶: {sin.shape}")
    
    # éªŒè¯ä¸‰è§’å‡½æ•°æ€§è´¨
    cos_sin_sum = cos**2 + sin**2
    error = (cos_sin_sum - 1).abs().max()
    print(f"\nğŸ” æ•°å­¦æ€§è´¨éªŒè¯ (cosÂ²Î¸ + sinÂ²Î¸ = 1):")
    print(f"  æœ€å¤§è¯¯å·®: {error.item():.3e}")
    print(f"  æ˜¯å¦æ¥è¿‘1 (è¯¯å·® < 1e-6): {'æ˜¯' if error < 1e-6 else 'å¦'}")

    # åˆ†æä½ç½®å·®å¼‚
    print("\nğŸŒ ä½ç½®ç¼–ç å·®å¼‚åˆ†æ:")
    for pos_diff in [0, 1, 4, 8]:
        # è®¡ç®—ä½ç½®å·®ä¸ºpos_diffæ—¶çš„ç‚¹ç§¯
        dot_products = []
        for i in range(0, seq - pos_diff):
            q = cos[0, i] * sin[0, i + pos_diff] - sin[0, i] * cos[0, i + pos_diff]
            dot_products.append(q.mean().item())
        
        avg_dot = np.mean(dot_products)
        print(f"  ä½ç½®å·® {pos_diff:2d}: å¹³å‡ç‚¹ç§¯ = {avg_dot:.4f}")

    # å¯è§†åŒ–éƒ¨åˆ†
    if visualize:
        try:
            import matplotlib.pyplot as plt
            # 1. é¢‘ç‡å‚æ•°å¯è§†åŒ–
            plt.figure(figsize=(12, 10))
            
            # é¢‘ç‡å‚æ•°
            plt.subplot(2, 1, 1)
            plt.plot(inv_freq, 'o-', markersize=3)
            plt.title("RoPE Frequency Parameters (inv_freq)")
            plt.xlabel("Dimension Index")
            plt.ylabel("Frequency Value")
            plt.grid(True)
            
            # 2. ä¸åŒä½ç½®çš„è§’åº¦å˜åŒ–
            plt.subplot(2, 1, 2)
            positions_to_plot = [0, 1, 4, 8]
            dims_to_plot = min(32, head_dim)
            
            # å–ç¬¬ä¸€ä¸ªbatchçš„ä¸åŒä½ç½®
            for pos in positions_to_plot:
                # è®¡ç®—è§’åº¦ (Î¸ = position * inv_freq)
                angles = (position_ids[0, pos] * rot.inv_freq).cpu().numpy()
                plt.plot(angles[:dims_to_plot], label=f"Position {pos}")
            
            plt.title(f"Angle Values for First {dims_to_plot} Dimensions")
            plt.xlabel("Dimension Index")
            plt.ylabel("Angle (radians)")
            plt.legend()
            plt.grid(True)
            plt.tight_layout()
            plt.savefig("rope_analysis.png")
            print("\nğŸ“ˆ å¯è§†åŒ–å·²ä¿å­˜è‡³ rope_analysis.png")
            
            # 3. ä½ç½®å·®å¼‚çƒ­åŠ›å›¾
            plt.figure(figsize=(10, 8))
            max_pos = 10  # åªæ˜¾ç¤ºå‰10ä¸ªä½ç½®
            
            # è®¡ç®—ç›¸å¯¹ä½ç½®ç¼–ç çš„å·®å¼‚
            position_diff = np.zeros((max_pos, max_pos))
            for i in range(max_pos):
                for j in range(max_pos):
                    # è®¡ç®—ç‚¹ç§¯ä½œä¸ºç›¸ä¼¼åº¦
                    sim = (cos[0, i] * cos[0, j] + sin[0, i] * sin[0, j]).mean().item()
                    position_diff[i, j] = sim
            
            plt.imshow(position_diff, cmap='viridis', origin='lower')
            plt.colorbar(label='Position Similarity')
            plt.title("Position Encoding Similarity Heatmap")
            plt.xlabel("Position j")
            plt.ylabel("Position i")
            plt.xticks(range(max_pos))
            plt.yticks(range(max_pos))
            plt.savefig("position_similarity.png")
            print("ğŸ“Š ä½ç½®ç›¸ä¼¼åº¦çƒ­åŠ›å›¾å·²ä¿å­˜è‡³ position_similarity.png")
            
        except ImportError:
            print("\nâš  æ— æ³•å¯¼å…¥ matplotlibï¼Œè·³è¿‡å¯è§†åŒ–éƒ¨åˆ†")
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆ!")

# æ‰§è¡Œæµ‹è¯•
if __name__ == "__main__":
    test_rotary_embedding(visualize=True)