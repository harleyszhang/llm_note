## 1. ä»‹ç»

DeepSeek-V2 æ˜¯ä¸€ç§é«˜æ•ˆçš„å¼€æºæ··åˆä¸“å®¶ï¼ˆMoEï¼‰è¯­è¨€æ¨¡å‹ï¼ŒåŸºäºåˆ›æ–°çš„ Transformer æ¶æ„ï¼Œå®ç°äº†ç»æµçš„è®­ç»ƒå’Œé«˜æ•ˆçš„æ¨ç†ã€‚DeepSeek-V2 å…·æœ‰ 2360 äº¿ä¸ªå‚æ•°(`236B`)ï¼Œæ¯ä¸ª token æ¿€æ´» 21 äº¿ä¸ªå‚æ•°ï¼Œæ”¯æŒ `128K` tokens çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚

å’Œ DeepSeekV1 æ¨¡å‹ç»“æ„æ²¿ç”¨ llama ç»“æ„ä¸åŒï¼ŒDeepSeekV2 æå‡ºäº†å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ï¼ˆ`MLA`ï¼‰å’Œ `DeepSeekMoE`ï¼Œæ—¨åœ¨ä¼˜åŒ– Transformer æ¡†æ¶ä¸­çš„æ³¨æ„åŠ›æ¨¡å—å’Œå‰é¦ˆç½‘ç»œï¼ˆFFNsï¼‰ã€‚
1. `MLA`: Multi-head Latent Attention ç»“æ„ï¼Œé€šè¿‡**ä½ç§©é”®å€¼è”åˆå‹ç¼©**ï¼Œ**å‡å°‘äº†æ¨ç†æ—¶çš„ KV ç¼“å­˜**ï¼Œä»è€Œæé«˜äº†æ¨ç†æ•ˆç‡ã€‚
2. `DeepSeekMoE`: `FFN`ï¼ˆæ ‡å‡† `MOE`ï¼‰ çš„ä¼˜åŒ–ç‰ˆã€‚
- **ç»†ç²’åº¦ä¸“å®¶åˆ’åˆ†(Routed Expert)**ï¼šç›¸æ¯”æ ‡å‡† MOEï¼ŒDeepSeekMoE åœ¨ä¿æŒå‚æ•°é‡ä¸å˜çš„å‰æä¸‹ï¼Œé€šè¿‡å‡å°æ¯ä¸ª Expert çš„ `FFN` ç»´åº¦ï¼Œæ¥å¢åŠ  Expert æ•°é‡ï¼Œè¿›è¡Œæ›´ç»†ç²’åº¦ä¸“å®¶åˆ’åˆ†ã€‚
- **å…±äº«ä¸“å®¶éš”ç¦»(Shared Expert)**: ç”¨äºè¡¨ç¤º Routed Expert ä¸­çš„å…±ç”¨çŸ¥è¯†ä¿¡æ¯ï¼Œå‡å°‘ Routed Expert çš„çŸ¥è¯†å†—ä½™é—®é¢˜ã€‚

DeepSeek-V2 æ¶æ„å›¾å¦‚ä¸‹æ‰€ç¤ºï¼š

![architecture_of_DeepSeekv2](../images/deepseekv2/architecture_of_DeepSeek-V2.png)

## 2. æ¶æ„

æœ¬èŠ‚ä»‹ç» MLA å’Œ DeepSeekMoE çš„è¯¦ç»†è®¾è®¡ã€‚

### 2.1 å¤šå¤´æ½œå˜é‡æ³¨æ„åŠ›ï¼ˆMLAï¼‰ï¼šæå‡æ¨ç†æ•ˆç‡

ä¼ ç»Ÿçš„ Transformer æ¨¡å‹é€šå¸¸é‡‡ç”¨å¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰ï¼Œä½†åœ¨ç”Ÿæˆï¼ˆgenerationï¼‰è¿‡ç¨‹ä¸­ï¼Œå…¶åºå¤§çš„ Key-Valueï¼ˆKVï¼‰ç¼“å­˜ä¼šæˆä¸ºé™åˆ¶æ¨ç†æ•ˆç‡çš„ç“¶é¢ˆã€‚ä¸ºå‡å°‘ KV ç¼“å­˜å ç”¨ï¼Œç ”ç©¶è€…æå‡ºäº†**å¤šæŸ¥è¯¢æ³¨æ„åŠ›**ï¼ˆMQAï¼‰ï¼ˆShazeer, 2019ï¼‰å’Œ**åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›**ï¼ˆGQAï¼‰ï¼ˆAinslie et al., 2023ï¼‰ã€‚è¿™ä¸¤ç§æ–¹æ³•è™½ç„¶å‡å°‘äº† KV ç¼“å­˜éœ€æ±‚ï¼Œä½†åœ¨æ€§èƒ½ä¸Šä»æ— æ³•ä¸ MHA ç›¸åª²ç¾ï¼ˆå…³äº MHAã€GQA å’Œ MQA çš„æ¶ˆèå®éªŒè§é™„å½• D.1ï¼‰ã€‚

DeepSeek-V2 å¼•å…¥äº†ä¸€ç§å…¨æ–°çš„æ³¨æ„åŠ›æœºåˆ¶**å¤šå¤´æ½œå˜é‡æ³¨æ„åŠ›**ï¼ˆ`MLA`ï¼‰ã€‚MLA ç»“åˆäº†**ä½ç§©é”®å€¼è”åˆå‹ç¼©**ï¼ˆlow-rank key-value joint compression,ï¼‰ï¼Œåœ¨æ¨ç†æ—¶å¤§å¹…é™ä½ KV ç¼“å­˜éœ€æ±‚ï¼ŒåŒæ—¶åœ¨æ€§èƒ½ä¸Šè¶…è¶Š MHAã€‚
> MLA æœ¬è´¨ä¸Šæ˜¯é€šè¿‡ä½ç§©è½¬æ¢çš„æ€è·¯å‡å°‘ head çš„ç»´åº¦ï¼Œå³æ¢ä¸ºä¸€ä¸ªå‹ç¼©çš„ QKVï¼Œå­˜å‚¨çš„KV çš„ç»´åº¦æ˜¾è‘—å‡å°ï¼Œè€Œä¸æ˜¯ GQA æ–¹æ³•å‡å°‘ kv heads çš„æ•°é‡ã€‚

#### 2.1.1 Standard Multi-Head Attention

å…ˆå›é¡¾ä¸‹æ ‡å‡†çš„**å¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰æœºåˆ¶**ã€‚è®¾ $d$ ä¸ºåµŒå…¥ç»´åº¦ï¼Œ$n_h$ ä¸ºæ³¨æ„åŠ›å¤´æ•°ï¼Œ$d_h$ ä¸ºå•ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦ï¼Œ$h_t \in R_d$ è¡¨ç¤ºç¬¬ $t$ ä¸ª token è¿›å…¥æ³¨æ„åŠ›å±‚çš„è¾“å…¥å‘é‡ã€‚

åœ¨ MHA æœºåˆ¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¸‰ä¸ªæŠ•å½±çŸ©é˜µ $W_Qã€W_Kã€W_V \in R^{n_h d_h\times d}$ åˆ†åˆ«è®¡ç®—å¾—åˆ°æŸ¥è¯¢å‘é‡ã€é”®å‘é‡å’Œå€¼å‘é‡ï¼ˆ$q_tã€k_tã€v_t \in R^{n_h d_h}$ï¼‰ï¼ŒQKV å‘é‡çš„çº¿æ€§å˜æ¢å…¬å¼å¦‚ä¸‹æ‰€ç¤ºï¼š
> QKV çš„çº¿æ€§å˜æ¢çš„æƒé‡çŸ©é˜µçš„ç¬¬äºŒä¸ªç»´åº¦å¤§å°ä¸€å®šä¸ºåµŒå…¥ç»´åº¦ $d$ã€‚

$$
\mathbf{q}_t = W^Q \mathbf{h}_t, \tag{1}
$$

$$
\mathbf{k}_t = W^K \mathbf{h}_t, \tag{2}
$$

$$
\mathbf{v}_t = W^V \mathbf{h}_t, \tag{3}
$$

ç„¶åï¼Œ$q_t$, $k_t$, $v_t$ å°†è¢«åˆ‡åˆ†ä¸º $n_h$ ä¸ªå¤´ï¼ˆ`heads`ï¼‰ï¼Œç”¨äºå¤šå¤´æ³¨æ„åŠ›è®¡ç®—ï¼š

$$[\mathbf{q}_{t,1}; \mathbf{q}_{t,2}; \dots; \mathbf{q}_{t,n_h}] = \mathbf{q}_t
\tag{4}$$

$$[\mathbf{k}_{t,1}; \mathbf{k}_{t,2}; \dots; \mathbf{k}_{t,n_h}] = \mathbf{k}_t
\tag{5}$$

$$[\mathbf{v}_{t,1}; \mathbf{v}_{t,2}; \dots; \mathbf{v}_{t,n_h}] = \mathbf{v}_t
\tag{6}$$

$$\mathbf{o}_{t,i} = \sum_{j=1}^{t} \text{Softmax}_j \left( \frac{\mathbf{q}_{t,i}^T \mathbf{k}_{j,i}}{\sqrt{d_h}} \right) \mathbf{v}_{j,i}
\tag{7}$$

$$\mathbf{u}_t = W^O [\mathbf{o}_{t,1}; \mathbf{o}_{t,2}; \dots; \mathbf{o}_{t,n_h}]
\tag{8}$$

å…¶ä¸­ï¼Œ$q_{t,i}$, $k_{t,i}$, $v_{t,i} \in \mathbb{R}^{d_h}$ åˆ†åˆ«è¡¨ç¤ºç¬¬ $i$ ä¸ªæ³¨æ„åŠ›å¤´çš„æŸ¥è¯¢ï¼ˆqueryï¼‰ã€é”®ï¼ˆkeyï¼‰å’Œå€¼ï¼ˆvalueï¼‰ï¼›$W_O \in \mathbb{R}^{d \times d_h n_h}$ è¡¨ç¤ºè¾“å‡ºæŠ•å½±çŸ©é˜µã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œkey å’Œ value éœ€è¦è¢«ç¼“å­˜ï¼Œä»¥åŠ é€Ÿè®¡ç®—ï¼Œé¿å…é‡å¤è®¡ç®—ã€‚

æ ‡å‡† `MHA` æ¯ä¸ª `token` çš„ kv ç¼“å†²å¤§å° = $2n_hd_h l$ï¼Œå•ä½ä¸ºå­—èŠ‚ `byte`ï¼›å¦‚æœä½¿ç”¨äº† `GQA` ä¼˜åŒ–æŠ€æœ¯ï¼Œæ¯ä¸ª token çš„ kv ç¼“å†²å¤§å°å˜ä¸º $2n_{kv}d_h l = 2n_hd_h l/\text{groups}$ ä¸ªå…ƒç´ ã€‚ä¸‹æ ‡ $t$ è¡¨ç¤ºç¬¬å‡ ä¸ª tokenï¼Œä¸‹æ ‡ $[1, n_h]$ è¡¨ç¤ºæ³¨æ„åŠ›å¤´æ•°ï¼Œ$l$ è¡¨ç¤º decoder layers æ•°ç›®ã€‚

åœ¨æ¨¡å‹éƒ¨ç½²æ—¶ï¼Œè¿™ç§åºå¤§çš„ KV ç¼“å­˜ æˆä¸ºäº†ä¸€ä¸ªä¸»è¦çš„ç“¶é¢ˆï¼Œé™åˆ¶äº†**æœ€å¤§æ‰¹é‡å¤§å°**ï¼ˆbatch sizeï¼‰å’Œ**åºåˆ—é•¿åº¦**ï¼ˆsequence lengthï¼‰ã€‚

#### 2.1.2 Low-Rank Key-Value Joint Compression

MLA çš„æ ¸å¿ƒæ˜¯å¯¹**é”®**ï¼ˆkeysï¼‰å’Œ**å€¼**ï¼ˆvaluesï¼‰è¿›è¡Œ**ä½ç§©è”åˆå‹ç¼©**ï¼ˆlow-rank joint compressionï¼‰ï¼Œä»¥å‡å°‘ KV ç¼“å­˜ï¼ˆKV cacheï¼‰çš„å ç”¨ï¼š

$$
\mathbf{c}_t^{KV} = W^{DKV} \mathbf{h}_t,
\tag{9}
$$

$$
\mathbf{k}_t^{C} = W^{UK} \mathbf{c}_t^{KV},
\tag{10}
$$

$$
\mathbf{v}_t^{C} = W^{UV} \mathbf{c}_t^{KV},
\tag{11}
$$

`KV` å‘é‡çš„ç”Ÿæˆæ˜¯å…ˆæŠ•å½±åˆ°ä¸€ä¸ª**ä½ç»´**ï¼ˆ`5120 -> 512`ï¼‰çš„ `compressed_kv` å‘é‡ï¼ˆ$\mathbf{c}_t^{KV}$ï¼‰å†å‡ç»´å±•å¼€å¾—åˆ° $\mathbf{k}_t^{C}$ å’Œ $\mathbf{v}_t^{C}$ã€‚ä¸Šè¿°å…¬å¼çš„å„ä¸ªå˜é‡å®šä¹‰ï¼š

- $\mathbf{c}_t^{KV}$ æ˜¯ `keys` å’Œ `values` çš„**å‹ç¼©åçš„æ½œåœ¨å‘é‡**ï¼ˆ`latent vector`ï¼‰ï¼›
- $d_c (\ll d_h n_h)$ ä»£è¡¨ `KV` å‹ç¼©ç»´åº¦ï¼ˆKV compression dimensionï¼‰
- $W^{DKV} \in \mathbb{R}^{d_c \times d}$ æ˜¯**é™ç»´æŠ•å½±çŸ©é˜µ**ï¼ˆdown-projection matrixï¼‰ï¼›
- $W^{UK}, W^{UV} \in \mathbb{R}^{d_h n_h \times d_c}$ åˆ†åˆ«æ˜¯ keys å’Œ values çš„**å‡ç»´æŠ•å½±çŸ©é˜µ**ï¼ˆup-projection matricesï¼‰ã€‚

å¦å¤–ï¼Œè™½ç„¶ä¸èƒ½å‡å°‘ KV Cache çš„å ç”¨ï¼Œä½†æ˜¯ä¸ºäº†**å‡å°‘è®­ç»ƒæ—¶çš„æ¿€æ´»å†…å­˜**ï¼ˆactivation memoryï¼‰ï¼ŒåŒæ ·ä¹Ÿå¯¹æŸ¥è¯¢ï¼ˆqueriesï¼‰ä¹Ÿè¿›è¡Œäº†**ä½ç§©å‹ç¼©**ï¼ˆlow-rank compressionï¼‰ã€‚åŒæ ·ä¹Ÿæ˜¯å…ˆæŠ•å½±åˆ°ä¸€ä¸ª**ä½ç»´**ï¼ˆ`5120 -> 1536`ï¼‰çš„ `compressed_kv` å‘é‡ï¼ˆ$\mathbf{c}_t^{Q}$ï¼‰å†å‡ç»´å±•å¼€å¾—åˆ° $\mathbf{q}_t^{C}$:

$$
\mathbf{c}_t^{Q} = W^{DQ} \mathbf{h}_t,
\tag{12}
$$

$$
\mathbf{q}_t^{C} = W^{UQ} \mathbf{c}_t^{Q},
\tag{13}
$$

ç±»æ¯”å‰é¢çš„å…¬å¼å¯çŸ¥:
- $\mathbf{c}_t^{Q} \in \mathbb{R}^{d'_c}$ æ˜¯æŸ¥è¯¢çš„å‹ç¼©æ½œåœ¨å‘é‡ï¼ˆcompressed latent vector for queriesï¼‰ï¼›
- $d'_c (\ll d_h n_h)$ è¡¨ç¤ºæŸ¥è¯¢çš„å‹ç¼©ç»´åº¦ï¼ˆquery compression dimensionï¼‰ï¼›
- $W^{DQ} \in \mathbb{R}^{d'_c \times d}$ æ˜¯æŸ¥è¯¢çš„é™ç»´æŠ•å½±çŸ©é˜µï¼›
- $W^{UQ} \in \mathbb{R}^{d_h n_h \times d'_c}$ æ˜¯æŸ¥è¯¢çš„å‡ç»´æŠ•å½±çŸ©é˜µï¼ˆup-projection matrixï¼‰ã€‚

#### 2.1.3 Decoupled Rotary Position Embedding

å’Œ DeepSeek 67Bï¼ˆDeepSeek-AI, 2024ï¼‰ç±»ä¼¼ï¼Œä½œè€…ä¹Ÿè®¡åˆ’åœ¨ DeepSeek-V2 ä¸­ä½¿ç”¨ æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPE, Rotary Position Embeddingï¼‰ï¼ˆSu et al., 2024ï¼‰ã€‚ä½†æ˜¯ï¼ŒRoPE ä¸**ä½ç§© KV å‹ç¼©ï¼ˆlow-rank KV compressionï¼‰å¹¶ä¸å…¼å®¹**ã€‚

å…·ä½“æ¥è¯´ï¼ŒRoPE ä½¿é”®ï¼ˆKeyï¼‰å’ŒæŸ¥è¯¢ï¼ˆQueryï¼‰éƒ½å…·å¤‡**ä½ç½®æ•æ„Ÿæ€§**ï¼ˆposition sensitivityï¼‰ã€‚å¦‚æœæˆ‘ä»¬åœ¨**å‹ç¼©åçš„é”®** $\mathbf{k}_t^{C}$ ä¸Šåº”ç”¨ ROPEï¼Œé‚£ä¹ˆå®é™…ä¸Šæˆ‘ä»¬å¾—åˆ°çš„é”®è¡¨ç¤ºä¼šæ˜¯è¿™æ ·çš„å½¢å¼ï¼š

$$k_t^R = \text{ROPE}(W^{UK} \mathbf{c}_t^{KV})$$


å¾ˆæ˜æ˜¾å¼ï¼ˆ10ï¼‰ä¸­çš„ $W^{UK}$ å’Œ RoPE æ—‹è½¬çŸ©é˜µåœ¨è®¡ç®—è¿‡ç¨‹ä¸­â€œè€¦åˆâ€åœ¨ä¸€èµ·â€”è¿™æ„å‘³ç€ $W^{UK}$ è¾“å‡ºçš„ç»“æœä¼šå§‹ç»ˆè¢«é‚£ä¸ªä¾èµ–äºå…·ä½“ä½ç½®çš„æ—‹è½¬çŸ©é˜µæ‰€â€œä¿®æ­£â€æˆ–â€œè°ƒåˆ¶â€ã€‚

è¿™æ ·ä¼šå¯¼è‡´åœ¨æ‰§è¡Œ atten weightï¼ˆ$QK^T$ï¼‰çš„è®¡ç®—ä¼˜åŒ–ä¸­ï¼Œæ— æ³•åƒåŸæœ¬è®¾æƒ³çš„é‚£æ ·ï¼ŒæŠŠ $W^{UK}$ å¸æ”¶åˆ° $W^Q$  ä¸­ï¼Œå› ä¸º å½“å‰ç”Ÿæˆ token ç›¸å…³çš„ RoPE çŸ©é˜µä½äº $W^Q$  å’Œ $W^{UK}$ ä¹‹é—´ï¼Œè€ŒçŸ©é˜µä¹˜æ³•ä¸æ»¡è¶³äº¤æ¢å¾‹ï¼ˆcommutative lawï¼‰ã€‚è¿™ç›´æ¥å¯¼è‡´åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¿…é¡»**é‡æ–°è®¡ç®—**æ‰€æœ‰ prefix token çš„é”®ï¼ˆkeysï¼‰ï¼Œè¿™å°†æ˜¾è‘—é™ä½æ¨ç†æ•ˆç‡ã€‚

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§**è§£è€¦ RoPE**ï¼ˆdecoupled RoPEï¼‰çš„ç­–ç•¥ï¼Œé€šè¿‡**é¢å¤–å¼•å…¥å¤šå¤´æŸ¥è¯¢**ï¼ˆmulti-head queriesï¼‰$q_{t,i}^R \in \mathbb{R}^{d^R_h}$å’Œ**é‡‡ç”¨ä¸€ä¸ªå…±äº«é”®**ï¼ˆshared key) $k_t^R \in \mathbb{R}^{d^R_h}$ æ¥**æ‰¿è½½ RoPE ä¿¡æ¯**ã€‚å…¶ä¸­ $d^R_h$  ä»£è¡¨**è§£è€¦æŸ¥è¯¢å’Œé”®çš„æ¯å¤´ç»´åº¦**ï¼ˆper-head dimension of the decoupled queries and keyï¼‰ã€‚

åœ¨ä½¿ç”¨**è§£è€¦ RoPE ç­–ç•¥**åï¼Œ`MLA` çš„è®¡ç®—è¿‡ç¨‹å˜æˆå¦‚ä¸‹æ‰€ç¤ºï¼š

$$
\left[ \mathbf{q}_{t,1}^{R}; \mathbf{q}_{t,2}^{R}; \dots; \mathbf{q}_{t,n_h}^{R} \right] = \mathbf{q}_t^{R} = \text{RoPE}(W^{QR} \mathbf{c}_t^{Q}),
\tag{14}
$$

$$
\mathbf{k}_t^{R} = \text{RoPE}(W^{KR} \mathbf{h}_t),
\tag{15}
$$

$$
\mathbf{q}_{t,i} = \left[ \mathbf{q}_{t,i}^{C}; \mathbf{q}_{t,i}^{R} \right],
\tag{16}
$$

$$
\mathbf{k}_{t,i} = \left[ \mathbf{k}_{t,i}^{C}; \mathbf{k}_{t,i}^{R} \right],
\tag{17}
$$

$$
\mathbf{o}_{t,i} = \sum_{j=1}^{t} \text{Softmax}_j \left( \frac{\mathbf{q}_{t,i}^{T} \mathbf{k}_{j,i}}{\sqrt{d_h + d_h^{R}}} \right) \mathbf{v}_{j,i}^{C},
\tag{18}
$$

$$
\mathbf{u}_t = W^{O} \left[ \mathbf{o}_{t,1}; \mathbf{o}_{t,2}; \dots; \mathbf{o}_{t,n_h} \right],
\tag{19}
$$

å…¶ä¸­ï¼Œ$W^{QR} \in \mathbb{R}^{d^R_h n_h \times d'_c}$ å’Œ $W^{KR} \in \mathbb{R}^{d^R_h \times d}$ åˆ†åˆ«æ˜¯ç”¨äºç”Ÿæˆ**è§£è€¦æŸ¥è¯¢ï¼ˆdecoupled queriesï¼‰å’Œè§£è€¦é”®ï¼ˆdecoupled keyï¼‰çš„çŸ©é˜µ**ã€‚$\text{RoPE}(\cdot)$ è¡¨ç¤ºåº”ç”¨ RoPE çŸ©é˜µçš„æ“ä½œï¼Œ$\cdot ; \cdot$ è¡¨ç¤ºæ‹¼æ¥ï¼ˆconcatenationï¼‰æ“ä½œã€‚

åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œè§£è€¦åçš„é”®ï¼ˆdecoupled keyï¼‰ä¹Ÿéœ€è¦ç¼“å­˜ã€‚å› æ­¤ï¼ŒDeepSeek-V2 çš„ KV ç¼“å­˜æ€»å¤§å°ä¸º $(d_c + d^R_h)l$ ä¸ªå…ƒç´ ã€‚

å¾ˆæ˜æ˜¾å’Œå‰é¢å…¬å¼ç›¸æ¯”ï¼Œå¤šäº† $\mathbf{q}_t^{R}$ å’Œ $\mathbf{k}_t^{R}$ ä¸¤ä¸ªå˜é‡çš„è®¡ç®—è¿‡ç¨‹ï¼Œå®ƒä»¬ç”¨äºå•ç‹¬æ‰¿è½½ ROPE ä¿¡æ¯ï¼Œå¹¶å’Œå‰é¢è®¡ç®—å¾—åˆ°çš„ $\mathbf{q}_t^{C}$ å’Œ $\mathbf{k}_t^{C}$ åšæ‹¼æ¥åå¾—åˆ°æ–°çš„ $qã€k$ï¼Œå†æ‰§è¡Œ atten weight è®¡ç®—ï¼ˆ$qk^t$ï¼‰ã€‚

æœ€åï¼Œæ€»ç»“ä¸‹å®Œæˆçš„ MLA è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹æ‰€ç¤ºï¼š

![Formulas_MLA](../images/deepseekv2/Formulas_MLA.png)

#### 2.1.4 kv cache å¤§å°çš„æ¯”è¾ƒ

ä¸‹è¡¨ 1 ä¸­å¯¹æ¯”äº†ä¸åŒæ³¨æ„åŠ›æœºåˆ¶ä¸‹ï¼Œæ¯ä¸ª token éœ€è¦çš„ KV ç¼“å­˜å¤§å°ã€‚MLA ä»…éœ€è¦**å°‘é‡çš„ KV ç¼“å­˜**ï¼Œå…¶å¤§å°ç›¸å½“äºä»…æœ‰ $2.25$ ç»„ï¼ˆgroupsï¼‰çš„ GQAï¼Œä½†å…¶æ€§èƒ½å´å¼ºäº MHAã€‚

![table1](../images/deepseekv2/table1.png)

è¡¨ 1ï½œä¸åŒæ³¨æ„åŠ›æœºåˆ¶ä¸‹ï¼Œæ¯ä¸ª token éœ€è¦çš„ KV ç¼“å­˜å¯¹æ¯”ã€‚å…¶ä¸­ï¼Œ
- $n_h$ è¡¨ç¤º**æ³¨æ„åŠ›å¤´çš„æ•°é‡**ï¼Œ
- $d_h$ è¡¨ç¤ºæ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦ï¼Œ
- $l$ è¡¨ç¤ºæ¨¡å‹å±‚æ•°ï¼Œ
- $n_g$ è¡¨ç¤º GQA çš„ç»„æ•°ï¼Œ
- **$d_c$ å’Œ $d^R_h$ åˆ†åˆ«è¡¨ç¤º MLA ä¸­çš„ KV å‹ç¼©ç»´åº¦å’Œè§£è€¦æŸ¥è¯¢ä¸é”®çš„ per-head ç»´åº¦**ã€‚

KV ç¼“å­˜çš„æ•°é‡ä»¥**å…ƒç´ (elements)ä¸ªæ•°**è®¡ç®—ï¼Œä¸è€ƒè™‘å­˜å‚¨ç²¾åº¦ï¼ˆstorage precisionï¼‰ã€‚å¯¹äº DeepSeek-V2ï¼Œ
- $d_c$ è®¾å®šä¸º  $4d_h$ï¼Œ
- $d^R_h$ è®¾å®šä¸º $\frac{d_h}{2}$ã€‚

å› æ­¤ï¼Œ**DeepSeek-V2 åªéœ€è¦ ç›¸å½“äº GQA $2.25$ ç»„çš„ KV ç¼“å­˜ï¼Œä½†ç›¸æ¯” MHA ä»èƒ½æä¾›æ›´å¼ºçš„æ€§èƒ½**ã€‚

### 2.2 DeepSeekMoEï¼šä»¥ç»æµæˆæœ¬è®­ç»ƒå¼ºå¤§çš„æ¨¡å‹

å¯¹äº FFNï¼ˆå‰é¦ˆç½‘ç»œï¼‰ï¼Œæˆ‘ä»¬é‡‡ç”¨ DeepSeekMoE æ¶æ„ï¼ˆDai et al., 2024ï¼‰ã€‚DeepSeekMoE ä¸»è¦åŒ…å«ä¸¤ä¸ªå…³é”®æ€æƒ³ï¼š
1. **æ›´ç²¾ç»†åœ°åˆ’åˆ†ä¸“å®¶ç½‘ç»œ**ï¼Œæå‡æ¯ä¸ªä¸“å®¶çš„ä¸“ä¸šæ€§ï¼Œæé«˜çŸ¥è¯†è¡¨è¾¾çš„å‡†ç¡®åº¦ã€‚
2. **å¼•å…¥éƒ¨åˆ†å…±äº«ä¸“å®¶**ï¼Œå‡å°‘ä¸åŒä¸“å®¶é—´çš„çŸ¥è¯†å†—ä½™ï¼Œä»è€Œæå‡è®¡ç®—æ•ˆç‡ã€‚

ç›¸æ¯”ä¼ ç»Ÿçš„ MoE æ¶æ„ï¼ˆå¦‚ GShardï¼ŒLepikhin et al., 2021ï¼‰ï¼Œ**DeepSeekMoE åœ¨ç›¸åŒçš„ä¸“å®¶å‚æ•°é‡å’Œæ¿€æ´»å‚æ•°é‡ä¸‹ï¼Œèƒ½æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½**ã€‚

è®¾ $u_t$ ä¸ºç¬¬ $t$ ä¸ª token çš„ FFN è¾“å…¥ï¼Œå…¶ FFN è¾“å‡º $h'_t$ è®¡ç®—å¦‚ä¸‹ï¼š

$$
\mathbf{h}_t' = \mathbf{u}_t + \sum_{i=1}^{N_s} \text{FFN}_i^{(s)} (\mathbf{u}_t) + \sum_{i=1}^{N_r} g_{ij,t} \text{FFN}_i^{(r)} (\mathbf{u}_t), \tag{20}
$$

$$
g_{ij,t} =
\begin{cases}
s_{ij,t}, & s_{ij,t} \in \text{Topk}(\{s_{ij,t}| 1 \leq j \leq N_r\}, K_r), \\
0, & \text{otherwise},
\end{cases} \tag{21}
$$

$$
s_{ij,t} = \text{Softmax}_i (\mathbf{u}_t^T e_i), \tag{22}
$$

ä¸Šè¿°å…¬å¼ä¸­ï¼š
- $N_s$ å’Œ $N_r$ åˆ†åˆ«è¡¨ç¤º**å…±äº«ä¸“å®¶**å’Œ**è·¯ç”±ä¸“å®¶**çš„æ•°é‡ï¼›
- $FFN(s) i(Â·)$ å’Œ $FFN(r) ğ‘–(Â·)$ åˆ†åˆ«è¡¨ç¤ºç¬¬ $i$ ä¸ªå…±äº«ä¸“å®¶å’Œç¬¬ $i$ ä¸ªè·¯ç”±ä¸“å®¶çš„è®¡ç®—è¿‡ç¨‹ï¼›
- $K_r$ è¡¨ç¤ºæ¿€æ´»çš„è·¯ç”±ä¸“å®¶æ•°é‡ï¼›
- $g_{i, t}$ æ˜¯ç¬¬ $i$ ä¸ªä¸“å®¶çš„é—¨æ§å€¼ï¼Œç”¨æ¥å†³å®šè¯¥ä¸“å®¶æ˜¯å¦æ¿€æ´»ï¼›
- $s_{i, t}$ æ˜¯ token åˆ°ä¸“å®¶çš„äº²å’Œåº¦å€¼ï¼Œè¡¨ç¤º token å’Œä¸“å®¶ä¹‹é—´çš„ç›¸å…³æ€§ï¼›
- $e_i$ æ˜¯è¯¥å±‚ç¬¬ $i$ ä¸ªè·¯ç”±ä¸“å®¶çš„è´¨å¿ƒï¼Œç”¨äºè¡¨ç¤ºä¸“å®¶çš„èšåˆç‰¹å¾ï¼›
- $\text{Topk}(Â·, K)$ è¡¨ç¤ºä»ç¬¬ $t$ ä¸ª token è®¡ç®—çš„æ‰€æœ‰è·¯ç”±ä¸“å®¶çš„äº²å’Œåº¦åˆ†æ•°ä¸­ï¼Œé€‰æ‹©å‡º $K$ ä¸ªæœ€é«˜çš„å€¼ï¼Œå¹¶å°†è¿™äº›åˆ†æ•°ç»„æˆä¸€ä¸ªé›†åˆã€‚

#### 2.2.2. è®¾å¤‡å—é™è·¯ç”±ï¼ˆDevice-Limited Routingï¼‰

ä½œè€…è®¾è®¡äº†ä¸€ç§è®¾å¤‡å—é™çš„è·¯ç”±æœºåˆ¶ï¼Œç”¨äºé™åˆ¶ MoE ç›¸å…³çš„é€šä¿¡æˆæœ¬ã€‚

å½“é‡‡ç”¨**ä¸“å®¶å¹¶è¡Œ**ï¼ˆexpert parallelismï¼‰æ—¶ï¼Œè·¯ç”±ä¸“å®¶ï¼ˆrouted expertsï¼‰ä¼šåˆ†å¸ƒåœ¨å¤šä¸ªè®¾å¤‡ä¸Šã€‚å¯¹äºæ¯ä¸ª tokenï¼Œå®ƒçš„ MoE **ç›¸å…³é€šä¿¡é¢‘ç‡**ä¸å…¶ç›®æ ‡ä¸“å®¶æ‰€æ¶‰åŠçš„è®¾å¤‡æ•°é‡æˆæ­£æ¯”ã€‚ç”±äº DeepSeekMoE é‡‡ç”¨äº†ç²¾ç»†çš„ä¸“å®¶åˆ’åˆ†ç­–ç•¥ï¼Œæ¿€æ´»çš„ä¸“å®¶æ•°é‡å¯èƒ½è¾ƒå¤šï¼Œå¦‚æœç›´æ¥åº”ç”¨ä¸“å®¶å¹¶è¡Œï¼Œä¼šå¯¼è‡´æ›´é«˜çš„ MoE ç›¸å…³é€šä¿¡æˆæœ¬ã€‚

åœ¨ DeepSeek-V2 ä¸­ï¼Œé™¤äº†ç›´æ¥é€‰æ‹©**å¾—åˆ†æœ€é«˜**çš„ $K$ ä¸ªè·¯ç”±ä¸“å®¶ï¼ˆtop-K selectionï¼‰ ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜ç¡®ä¿æ¯ä¸ª token çš„ç›®æ ‡ä¸“å®¶æœ€å¤šåˆ†å¸ƒåœ¨ $M$ å°è®¾å¤‡ä¸Šã€‚å…·ä½“è€Œè¨€ï¼Œå¯¹äºæ¯ä¸ª tokenï¼Œæˆ‘ä»¬é¦–å…ˆé€‰æ‹©**æ‹¥æœ‰æœ€é«˜äº²å’Œåº¦åˆ†æ•°çš„** $M$ å°è®¾å¤‡ï¼Œç„¶ååœ¨è¿™ $M$ å°è®¾å¤‡ä¸Šçš„ä¸“å®¶ä¸­æ‰§è¡Œ `top-K` é€‰æ‹©ã€‚å®è·µä¸­ï¼Œæˆ‘ä»¬å‘ç°å½“ $M \geq 3$ æ—¶ï¼Œè®¾å¤‡å—é™è·¯ç”±çš„æ•ˆæœå¯ä»¥å¤§è‡´åŒ¹é…ä¸å—é™çš„ top-K è·¯ç”±ã€‚

#### 2.2.3. è´Ÿè½½å‡è¡¡çš„è¾…åŠ©æŸå¤±ï¼ˆAuxiliary Loss for Load Balanceï¼‰

ä½œè€…åœ¨è‡ªåŠ¨å­¦ä¹ çš„è·¯ç”±ç­–ç•¥ä¸­å¼•å…¥äº†è´Ÿè½½å‡è¡¡æœºåˆ¶ã€‚
1. è´Ÿè½½ä¸å‡è¡¡ä¼šå¯¼è‡´è·¯ç”±å¡Œé™·ï¼ˆrouting collapseï¼‰ï¼ˆShazeer et al., 2017ï¼‰ï¼Œå³éƒ¨åˆ†ä¸“å®¶å¯èƒ½æ— æ³•å¾—åˆ°å……åˆ†è®­ç»ƒå’Œåˆ©ç”¨ã€‚
2. åœ¨ä¸“å®¶å¹¶è¡Œï¼ˆexpert parallelismï¼‰æœºåˆ¶ä¸‹ï¼Œè´Ÿè½½ä¸å‡è¡¡ä¼šé™ä½è®¡ç®—æ•ˆç‡ã€‚

åœ¨ DeepSeek-V2 è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸‰ç§**è¾…åŠ©æŸå¤±**ï¼ˆauxiliary lossesï¼‰ï¼Œåˆ†åˆ«ç”¨äºæ§åˆ¶ï¼š
- ä¸“å®¶çº§è´Ÿè½½å‡è¡¡ï¼ˆ $L_{\text{ExpBal}}$ ï¼‰ï¼Œ
- è®¾å¤‡çº§è´Ÿè½½å‡è¡¡ï¼ˆ $L_{\text{DevBal}}$ ï¼‰ï¼Œ
- é€šä¿¡å‡è¡¡ï¼ˆ $L_{\text{CommBal}}$ ï¼‰ã€‚

è¿™ä¸‰ç§æŸå¤±å‡½æ•°ååŒä½œç”¨ï¼Œç¡®ä¿ DeepSeek-V2 åœ¨è®¡ç®—èµ„æºå—é™çš„æ¡ä»¶ä¸‹ï¼Œä»èƒ½é«˜æ•ˆè®­ç»ƒé«˜æ€§èƒ½ MoE æ¨¡å‹ã€‚

#### 2.2.4. Token-Dropping ç­–ç•¥

è™½ç„¶**å¹³è¡¡æŸ**å¤±æœ‰åŠ©äºå®ç°è´Ÿè½½å¹³è¡¡ï¼Œä½†å®ƒæ— æ³•å®Œå…¨ä¿è¯è´Ÿè½½çš„ä¸¥æ ¼å¹³è¡¡ã€‚ä¸ºäº†è§£å†³è´Ÿè½½ä¸å¹³è¡¡å¸¦æ¥çš„è®¡ç®—æµªè´¹ï¼Œä½œè€…åœ¨è®­ç»ƒä¸­å¼•å…¥äº†**è®¾å¤‡çº§çš„ Token-Dropping ç­–ç•¥**ã€‚è¯¥ç­–ç•¥é¦–å…ˆè®¡ç®—æ¯ä¸ªè®¾å¤‡çš„å¹³å‡è®¡ç®—é¢„ç®—ï¼Œå°†æ¯ä¸ªè®¾å¤‡çš„å®¹é‡å› å­è®¾ä¸º 1.0ã€‚ç„¶åï¼Œå€Ÿé‰´ Riquelme ç­‰äººï¼ˆ2021ï¼‰çš„æ€è·¯ï¼Œæˆ‘ä»¬ä¼šä¸¢å¼ƒæ¯ä¸ªè®¾å¤‡ä¸Šäº²å’Œåº¦æœ€ä½çš„ tokenï¼Œç›´åˆ°è¾¾æˆè®¡ç®—é¢„ç®—ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç¡®ä¿çº¦ 10% çš„è®­ç»ƒåºåˆ—ä¸­çš„ token ä¸ä¼šè¢«ä¸¢å¼ƒã€‚è¿™æ ·ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çµæ´»åœ°æ ¹æ®æ•ˆç‡éœ€æ±‚é€‰æ‹©æ˜¯å¦ä¸¢å¼ƒ tokenï¼ŒåŒæ—¶ä¿æŒè®­ç»ƒå’Œæ¨ç†çš„ä¸€è‡´æ€§ã€‚

## 3. ä»£ç å®ç°

### 3.1 MLA ä»£ç å®ç°è§£è¯»

DeepDeekv2 çš„æ¨¡å‹é…ç½®å¦‚ä¸‹æ‰€ç¤º:

<div align="center">
<img src="../images/deepseekv2/deepseekv2_config.png" width="40%" alt="deepseekv2_config">
</div>

#### 3.1.1 Q å‘é‡è®¡ç®—

1ï¼Œåœ¨ DeepSeek-V2 ä¸­ï¼ŒQ å‘é‡ä¹Ÿé‡‡ç”¨äº†ä½ç§©å‹ç¼©çš„æ–¹å¼ã€‚é¦–å…ˆï¼Œå°†è¾“å…¥å‘é‡æŠ•å½±åˆ°ä¸€ä¸ª 1536 ç»´çš„ä½ç»´ç©ºé—´ã€‚

ç„¶åï¼Œå†å°†å…¶æŠ•å½±åˆ° $\mathbb{R}^{H \times 128}$ çš„å¤šå¤´å‘é‡ç©ºé—´ä¸Šï¼ˆå…¶ä¸­ $H=128$ æ˜¯ `heads` æ•°ï¼‰ï¼Œå¾—åˆ°äº† Q å‘é‡çš„ç¬¬ä¸€éƒ¨åˆ†ã€‚

å†å°†å…¶æŠ•å½±åˆ° $\mathbb{R}^{H \times 64}$ ä¸Šå¹¶ä½¿ç”¨ RoPE åµŒå…¥ä½ç½®ä¿¡æ¯ï¼Œå¾—åˆ° Q å‘é‡çš„ç¬¬äºŒéƒ¨åˆ†ï¼›

æœ€åï¼Œå°†è¿™ä¸¤éƒ¨åˆ†è¿›è¡Œ `concat` æ‹¼æ¥å¾—åˆ°æœ€ç»ˆçš„ $Q$ å‘é‡ï¼š

$$ q_t = [q_t^C, q_t^R] \in \mathbb{R}^{B \times L \times H \times 192}$$

#### 3.1.2 KV å‘é‡è®¡ç®—

è®¡ç®— KV å‘é‡æ—¶ï¼Œé¦–å…ˆï¼Œå°†è¾“å…¥å‘é‡æŠ•å½±åˆ°ä¸€ä¸ª 512 ç»´çš„ä½ç»´ç©ºé—´ã€‚

ç„¶åï¼Œå’Œ Q å‘é‡çš„è®¡ç®—è¿‡ç¨‹ç±»ä¼¼ï¼Œå†å°†å…¶æŠ•å½±åˆ° $\mathbb{R}^{H \times 128}$ çš„å¤šå¤´å‘é‡ç©ºé—´ä¸Šï¼ˆå…¶ä¸­ $H=128$ æ˜¯ `heads` æ•°ï¼‰ï¼Œå¾—åˆ°äº† $K$ å‘é‡çš„ç¬¬ä¸€éƒ¨åˆ†ã€‚

$K$ çš„ç¬¬äºŒéƒ¨åˆ†åŒæ ·ä¹Ÿæ˜¯å°†è¾“å…¥å‘é‡æŠ•å½±åˆ° 64 ç»´å‘é‡ç©ºé—´å¹¶æ–½åŠ  RoPE åµŒå…¥ä½ç½®ä¿¡æ¯ã€‚

æœ€åï¼Œå’Œ Q ä¸åŒçš„æ˜¯ï¼Œå®Œæ•´çš„ K æ˜¯å°† K çš„ç¬¬äºŒéƒ¨åˆ†å¹¿æ’­åˆ°æ¯ä¸ª head åä¸ç¬¬ä¸€éƒ¨åˆ†æ‹¼æ¥å¾—åˆ°ï¼š

$$k_t = \begin{bmatrix}
    k_{t,1}^C & k_t^R \\ 
    k_{t,2}^C & k_t^R \\
    \vdots & \vdots \\
    \end{bmatrix} \in \mathbb{R}^{B \times L \times H \times 192}$$

ä¸Šè¿°å¹¿æ’­åæ‹¼æ¥çš„æ–¹å¼æ„å‘³ç€ï¼Œ**æ¯ä¸ª head çš„ RoPE éƒ¨åˆ†æ˜¯å®Œå…¨ç›¸åŒçš„**ã€‚

$V$ å‘é‡å› ä¸ºä¸éœ€è¦æ‰§è¡Œ ROPE æ“ä½œï¼Œæ‰€ä»¥å®ƒçš„çš„è®¡ç®—è¾ƒä¸ºç®€å•ï¼Œç›´æ¥å°† $c_t^{KV}$ è§£å‹ç¼©ï¼ˆå‡ç»´ï¼‰åˆ° $\mathbb{R}^{H \times 128}$ å³å¯ï¼š

$$ v_t = W^{UV} c_t^{KV} \in \mathbb{R}^{B \times L \times H \times 128} $$


#### 3.1.3 Self-Attention è®¡ç®—

Self-Attention çš„è®¡ç®—è¿‡ç¨‹å’Œä¼ ç»Ÿçš„ `MHA` ä¸€æ¨¡ä¸€æ ·ã€‚åŒæ ·ä¹Ÿæ˜¯é¦–å…ˆè®¡ç®— `attention score`ï¼š

$$a = \mathrm{softmax}\left(\frac{q_t^\top k_t + \mathrm{Mask}}{\sqrt{192}}\right) = 
\mathrm{softmax}\left(\frac{{q_t^C}^\top k_t^C + {q_t^R}^\top k_t^R + \mathrm{Mask}}{\sqrt{128 + 64}} \right)
\in \mathbb{R}^{B \times L \times H \times L} $$

è®¡ç®—å¯¹ $V$çš„åŠ æƒå’Œï¼Œå¹¶å°†æ‰€æœ‰ heads å‹å¹³ï¼ˆå³ heads * head_dimï¼‰ï¼Œå¾—åˆ° Attention è¾“å‡ºï¼š

$$ o = a \cdot v_t \in \mathbb{R}^{B \times L \times H \times 128} \cong \mathbb{R}^{B \times L \times 16384} $$

å…¶ä¸­ï¼Œ$16384 = 128 \times 128 = \text{num\;attention\;heads * v\;head\;dim}$ã€‚æœ€åï¼Œç»è¿‡å¦ä¸€ä¸ªæ³¨æ„åŠ›è¾“å‡ºçŸ©é˜µçš„æŠ•å½±ï¼ˆ5120 æ˜¯ `hidden_size`ï¼‰ï¼Œå°±èƒ½å¾—åˆ° MLA çš„æœ€ç»ˆè¾“å‡ºï¼š

$$u = W^O o \in \mathbb{R}^{B \times L \times 5120}$$

### transformers ä»£ç å®ç°è§£è¯»

transformers åº“ä¸­çš„ modeling_deepseek.py æ˜¯æ²¡æœ‰ç»è¿‡æ¨ç†åŠ é€Ÿä¼˜åŒ–çš„åŸå§‹å®ç°ï¼Œä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
# ä» LlamaAttention ä¿®æ”¹è€Œæ¥ï¼Œé€‚é… DeepseekV2 æ¨¡å‹çš„æ³¨æ„åŠ›æ¨¡å—
class DeepseekV2Attention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config: DeepseekV2Config, layer_idx: Optional[int] = None):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx  # å½“å‰å±‚ç´¢å¼•ï¼Œç”¨äºç¼“å­˜åŒºåˆ†
        if layer_idx is None:
            logger.warning_once("æœªæä¾› layer_idxï¼Œå¯èƒ½å¯¼è‡´ç¼“å­˜é”™è¯¯")

        # åŸºç¡€å‚æ•°
        self.attention_dropout = config.attention_dropout  # æ³¨æ„åŠ›æƒé‡ Dropout æ¦‚ç‡
        self.hidden_size = config.hidden_size              # éšè—å±‚ç»´åº¦
        self.num_heads = config.num_attention_heads        # æ³¨æ„åŠ›å¤´æ•°
        
        # RoPE ç›¸å…³å‚æ•°ï¼šqk_nope_head_dim": 128, "qk_rope_head_dim": 64, v_head_dim = 128,
        # "num_attention_heads": 128, "num_key_value_heads": 128,
        self.max_position_embeddings = config.max_position_embeddings  # æœ€å¤§ä½ç½®ç¼–ç é•¿åº¦
        self.rope_theta = config.rope_theta                            # RoPE åŸºé¢‘å‚æ•°
        self.qk_rope_head_dim = config.qk_rope_head_dim                # RoPE åº”ç”¨çš„å¤´ç»´åº¦
        
        # LoRA å‚æ•°
        self.q_lora_rank = config.q_lora_rank          # Query ä½ç§©çŸ©é˜µçš„ç§©
        self.kv_lora_rank = config.kv_lora_rank        # Key-Value ä½ç§©çŸ©é˜µçš„ç§©
        self.qk_nope_head_dim = config.qk_nope_head_dim  # éä½ç½®ç¼–ç çš„å¤´ç»´åº¦
        self.v_head_dim = config.v_head_dim            # Value å¤´ç»´åº¦
        self.q_head_dim = self.qk_nope_head_dim + self.qk_rope_head_dim  # Query æ€»å¤´ç»´åº¦

        self.is_causal = True  # æ˜¯å¦å› æœæ³¨æ„åŠ›ï¼ˆå±è”½æœªæ¥ä¿¡æ¯ï¼‰

        # Query æŠ•å½±å±‚ï¼ˆLoRA åˆ†è§£ä¸º q_a_proj å’Œ q_b_projï¼‰
        self.q_a_proj = nn.Linear(self.hidden_size, self.q_lora_rank, bias=config.attention_bias)
        self.q_a_layernorm = DeepseekV2RMSNorm(self.q_lora_rank)  # LoRA åçš„å½’ä¸€åŒ–
        self.q_b_proj = nn.Linear(self.q_lora_rank, self.num_heads * self.q_head_dim, bias=False)

        # Key-Value æŠ•å½±å±‚ï¼ˆLoRA åˆ†è§£ä¸º kv_a_proj_with_mqa å’Œ kv_b_projï¼‰
        self.kv_a_proj_with_mqa = nn.Linear(
            self.hidden_size,
            self.kv_lora_rank + self.qk_rope_head_dim,  # åŒ…å« RoPE çš„ Key éƒ¨åˆ†
            bias=config.attention_bias,
        )
        self.kv_a_layernorm = DeepseekV2RMSNorm(self.kv_lora_rank)
        self.kv_b_proj = nn.Linear(
            self.kv_lora_rank,
            self.num_heads * (self.qk_nope_head_dim + self.v_head_dim),  # åˆå¹¶ Key å’Œ Value
            bias=False,
        )

        # è¾“å‡ºæŠ•å½±å±‚
        self.o_proj = nn.Linear(
            self.num_heads * self.v_head_dim, self.hidden_size, bias=config.attention_bias
        )
        # æ³¨æ„åŠ›ç¼©æ”¾å› å­ï¼ˆè€ƒè™‘ RoPE ç¼©æ”¾é…ç½®ï¼‰
        self.softmax_scale = self.q_head_dim ** (-0.5)

        # åˆå§‹åŒ– RoPE
        self._init_rope()        
        if self.config.rope_scaling is not None:
            scaling_factor = self.config.rope_scaling["factor"]
            if self.config.rope_scaling.get("mscale_all_dim", 0):
                mscale = yarn_get_mscale(scaling_factor, self.config.rope_scaling["mscale_all_dim"])
                self.softmax_scale *= mscale ** 2

    ########çœç•¥äº† _init_ropeã€_shape æˆå‘˜å‡½æ•°ä»£ç ############
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        **kwargs,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        # è¾“å…¥å½¢çŠ¶æ£€æŸ¥
        bsz, q_len, _ = hidden_states.size()

        # 1. è®¡ç®— Query
        q = self.q_a_proj(hidden_states)  # LoRA æŠ•å½± [bsz, q_len, q_lora_rank]
        q = self.q_a_layernorm(q)         # å½’ä¸€åŒ–
        q = self.q_b_proj(q)              # å‡ç»´ [bsz, q_len, num_heads * q_head_dim]
        q = q.view(bsz, q_len, self.num_heads, self.q_head_dim).transpose(1, 2)
        q_nope, q_pe = torch.split(q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)

        # 2. è®¡ç®— Key-Value
        compressed_kv = self.kv_a_proj_with_mqa(hidden_states)  # [bsz, q_len, kv_lora_rank + qk_rope_head_dim]
        compressed_kv, k_pe = torch.split(compressed_kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)
        k_pe = k_pe.view(bsz, q_len, 1, self.qk_rope_head_dim).transpose(1, 2)  # æ‰©å±•ä¸ºå¤šå¤´
        
        # ä½ç§©æŠ•å½± Key-Value
        kv = self.kv_b_proj(self.kv_a_layernorm(compressed_kv))
        kv = kv.view(bsz, q_len, self.num_heads, self.qk_nope_head_dim + self.v_head_dim).transpose(1, 2)
        k_nope, value_states = torch.split(kv, [self.qk_nope_head_dim, self.v_head_dim], dim=-1)

        # 3. åº”ç”¨ RoPE ä½ç½®ç¼–ç 
        kv_seq_len = value_states.shape[-2]
        if past_key_value is not None:
            # åˆå¹¶å†å²ç¼“å­˜é•¿åº¦
            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)
        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)
        q_pe, k_pe = apply_rotary_pos_emb(q_pe, k_pe, cos, sin, position_ids)

        # 4. åˆå¹¶ä½ç½®ç¼–ç ä¸éä½ç½®ç¼–ç éƒ¨åˆ†
        query_states = torch.cat([q_nope, q_pe], dim=-1)
        key_states = torch.cat([k_nope, k_pe], dim=-1)

        # 5. æ›´æ–°ç¼“å­˜ï¼ˆè‹¥å¯ç”¨ï¼‰
        if past_key_value is not None:
            key_states, value_states = past_key_value.update(
                key_states, value_states, self.layer_idx, {"sin": sin, "cos": cos}
            )

        ###############è¿™æ­¥å¼€å§‹å’Œæ ‡å‡† attention çš„å®ç°ä»£ç ä¸€æ ·###################
        # 6. è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.softmax_scale
        if attention_mask is not None:
            attn_weights = attn_weights + attention_mask  # åº”ç”¨æ©ç 
        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)

        # 7. æ³¨æ„åŠ›åŠ æƒæ±‚å’Œ
        attn_output = torch.matmul(attn_weights, value_states)
        attn_output = attn_output.transpose(1, 2).contiguous().reshape(bsz, q_len, self.num_heads * self.v_head_dim)
        attn_output = self.o_proj(attn_output)  # è¾“å‡ºæŠ•å½±

        return attn_output, attn_weights if output_attentions else None, past_key_value
```


## å‚è€ƒèµ„æ–™

- [DeepSeek-V2 è®ºæ–‡](https://arxiv.org/pdf/2405.04434)
- [DeepSeek-V2é«˜æ€§èƒ½æ¨ç†ä¼˜åŒ–ç¬”è®°ï¼šMLAä¼˜åŒ–](https://github.com/madsys-dev/deepseekv2-profile/blob/main/workspace/blog/optimizing-mla.md)