"""
DeepSeek V3 MoE é€šä¿¡é‡åˆ†ææµ‹è¯•ç‰ˆæœ¬
"""

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

def analyze_moe_communication():
    """åˆ†æMoEé€šä¿¡é‡"""
    print("ğŸš€ DeepSeek V3 MoE é€šä¿¡é‡åˆ†æ")
    print("=" * 60)
    
    # åŸºç¡€é…ç½®
    total_experts = 256
    experts_per_token = 8
    hidden_size = 7168
    dtype_size = 2  # BF16
    
    # Prefillé˜¶æ®µé…ç½®
    prefill_ep_size = 32
    prefill_nodes = 4
    prefill_gpus_per_node = 8
    
    # Decodeé˜¶æ®µé…ç½®
    decode_ep_size = 144
    decode_nodes = 18
    decode_gpus_per_node = 8
    
    # æµ‹è¯•å‚æ•°
    batch_size = 1024
    prefill_seq_len = 2048
    decode_seq_len = 1
    
    print(f"ğŸ“‹ åŸºç¡€é…ç½®:")
    print(f"   - æ€»ä¸“å®¶æ•°: {total_experts}")
    print(f"   - æ¯ä¸ªtokené€‰æ‹©ä¸“å®¶æ•°: {experts_per_token}")
    print(f"   - éšè—å±‚ç»´åº¦: {hidden_size}")
    print(f"   - æ‰¹æ¬¡å¤§å°: {batch_size}")
    
    # Prefillé˜¶æ®µåˆ†æ
    print(f"\nğŸ“Š Prefillé˜¶æ®µåˆ†æ:")
    print(f"   - ä¸“å®¶å¹¶è¡Œåº¦: {prefill_ep_size}")
    print(f"   - èŠ‚ç‚¹æ•°: {prefill_nodes}")
    print(f"   - æ¯èŠ‚ç‚¹GPUæ•°: {prefill_gpus_per_node}")
    
    prefill_total_tokens = batch_size * prefill_seq_len
    prefill_total_assignments = prefill_total_tokens * experts_per_token
    prefill_tokens_per_expert = prefill_total_assignments / total_experts
    prefill_experts_per_gpu = total_experts // prefill_ep_size
    prefill_tokens_per_gpu = prefill_tokens_per_expert * prefill_experts_per_gpu
    
    prefill_send_volume_gb = prefill_tokens_per_gpu * hidden_size * dtype_size / (1024**3)
    prefill_total_volume_gb = prefill_send_volume_gb * 2
    
    print(f"   - æ€»tokenæ•°: {prefill_total_tokens:,}")
    print(f"   - æ¯ä¸“å®¶tokenæ•°: {prefill_tokens_per_expert:.0f}")
    print(f"   - æ¯GPUä¸“å®¶æ•°: {prefill_experts_per_gpu}")
    print(f"   - æ¯GPU tokenæ•°: {prefill_tokens_per_gpu:.0f}")
    print(f"   - æ¯GPUå‘é€é‡: {prefill_send_volume_gb:.2f} GB")
    print(f"   - æ€»é€šä¿¡é‡: {prefill_total_volume_gb:.2f} GB")
    
    # Decodeé˜¶æ®µåˆ†æ
    print(f"\nğŸ“Š Decodeé˜¶æ®µåˆ†æ:")
    print(f"   - ä¸“å®¶å¹¶è¡Œåº¦: {decode_ep_size}")
    print(f"   - èŠ‚ç‚¹æ•°: {decode_nodes}")
    print(f"   - æ¯èŠ‚ç‚¹GPUæ•°: {decode_gpus_per_node}")
    
    decode_total_tokens = batch_size * decode_seq_len
    decode_total_assignments = decode_total_tokens * experts_per_token
    decode_tokens_per_expert = decode_total_assignments / total_experts
    decode_experts_per_gpu = total_experts // decode_ep_size
    decode_tokens_per_gpu = decode_tokens_per_expert * decode_experts_per_gpu
    
    decode_send_volume_gb = decode_tokens_per_gpu * hidden_size * dtype_size / (1024**3)
    decode_total_volume_gb = decode_send_volume_gb * 2
    
    print(f"   - æ€»tokenæ•°: {decode_total_tokens:,}")
    print(f"   - æ¯ä¸“å®¶tokenæ•°: {decode_tokens_per_expert:.0f}")
    print(f"   - æ¯GPUä¸“å®¶æ•°: {decode_experts_per_gpu}")
    print(f"   - æ¯GPU tokenæ•°: {decode_tokens_per_gpu:.0f}")
    print(f"   - æ¯GPUå‘é€é‡: {decode_send_volume_gb:.4f} GB")
    print(f"   - æ€»é€šä¿¡é‡: {decode_total_volume_gb:.4f} GB")
    
    # å¯¹æ¯”åˆ†æ
    print(f"\nğŸ“ˆ å¯¹æ¯”åˆ†æ:")
    volume_ratio = prefill_total_volume_gb / decode_total_volume_gb
    print(f"   - é€šä¿¡é‡æ¯”ä¾‹: {volume_ratio:.0f}:1")
    print(f"   - Prefillé€šä¿¡é‡æ˜¯Decodeçš„ {volume_ratio:.0f} å€")
    
    return {
        'prefill': {
            'ep_size': prefill_ep_size,
            'nodes': prefill_nodes,
            'experts_per_gpu': prefill_experts_per_gpu,
            'tokens_per_gpu': prefill_tokens_per_gpu,
            'send_volume_gb': prefill_send_volume_gb,
            'total_volume_gb': prefill_total_volume_gb
        },
        'decode': {
            'ep_size': decode_ep_size,
            'nodes': decode_nodes,
            'experts_per_gpu': decode_experts_per_gpu,
            'tokens_per_gpu': decode_tokens_per_gpu,
            'send_volume_gb': decode_send_volume_gb,
            'total_volume_gb': decode_total_volume_gb
        }
    }

def visualize_all2all_process():
    """å¯è§†åŒ–All2Allé€šä¿¡è¿‡ç¨‹"""
    print("\nğŸ¨ ç”ŸæˆAll2Allå¯è§†åŒ–å›¾...")
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
    
    # Prefillé˜¶æ®µå¯è§†åŒ–
    ep_size = 32
    nodes = 4
    gpus_per_node = 8
    
    print(f"ğŸ“Š ç»˜åˆ¶Prefillé˜¶æ®µ (EP{ep_size}, {nodes}èŠ‚ç‚¹)")
    
    for node in range(nodes):
        for gpu in range(gpus_per_node):
            x = node * 3
            y = gpu * 2
            
            # GPUæ¡†
            rect = patches.Rectangle((x-0.4, y-0.4), 0.8, 0.8, 
                                   linewidth=2, edgecolor='black', 
                                   facecolor='lightblue', alpha=0.7)
            ax1.add_patch(rect)
            ax1.text(x, y, f'{node}-{gpu}', ha='center', va='center', fontsize=8)
            
            # ä¸“å®¶åˆ†å¸ƒ (æ¯GPU 8ä¸ªä¸“å®¶)
            experts_per_gpu = 256 // ep_size
            for j in range(experts_per_gpu):
                ex = x - 0.3 + j * 0.15
                ey = y + 0.6
                circle = patches.Circle((ex, ey), 0.05, 
                                      facecolor='orange', edgecolor='black')
                ax1.add_patch(circle)
                ax1.text(ex, ey, f'E{j}', ha='center', va='center', fontsize=6)
    
    ax1.set_xlim(-1, nodes * 3 - 1)
    ax1.set_ylim(-1, gpus_per_node * 2 - 1)
    ax1.set_title('Prefillé˜¶æ®µä¸“å®¶åˆ†å¸ƒ (EP32, 4èŠ‚ç‚¹)', fontsize=14)
    ax1.set_xlabel('èŠ‚ç‚¹', fontsize=12)
    ax1.set_ylabel('GPU', fontsize=12)
    ax1.grid(True, alpha=0.3)
    
    # Decodeé˜¶æ®µå¯è§†åŒ–
    ep_size = 144
    nodes = 18
    gpus_per_node = 8
    
    print(f"ğŸ“Š ç»˜åˆ¶Decodeé˜¶æ®µ (EP{ep_size}, {nodes}èŠ‚ç‚¹)")
    
    # ç®€åŒ–çš„GPUåˆ†å¸ƒ (åªæ˜¾ç¤ºå‰32ä¸ªGPU)
    for i in range(min(ep_size, 32)):
        node = i // gpus_per_node
        gpu = i % gpus_per_node
        x = node * 2
        y = gpu * 1.5
        
        rect = patches.Rectangle((x-0.3, y-0.3), 0.6, 0.6, 
                               linewidth=2, edgecolor='black', 
                               facecolor='lightgreen', alpha=0.7)
        ax2.add_patch(rect)
        ax2.text(x, y, f'{node}-{gpu}', ha='center', va='center', fontsize=6)
        
        # ä¸“å®¶åˆ†å¸ƒ (æ¯GPU 1-2ä¸ªä¸“å®¶)
        experts_per_gpu = 256 // ep_size
        for j in range(experts_per_gpu):
            ex = x - 0.2 + j * 0.1
            ey = y + 0.4
            circle = patches.Circle((ex, ey), 0.03, 
                                  facecolor='red', edgecolor='black')
            ax2.add_patch(circle)
            ax2.text(ex, ey, f'E{j}', ha='center', va='center', fontsize=5)
    
    ax2.set_xlim(-1, min(nodes, 8) * 2 - 1)
    ax2.set_ylim(-1, gpus_per_node * 1.5 - 1)
    ax2.set_title('Decodeé˜¶æ®µä¸“å®¶åˆ†å¸ƒ (EP144, 18èŠ‚ç‚¹)', fontsize=14)
    ax2.set_xlabel('èŠ‚ç‚¹', fontsize=12)
    ax2.set_ylabel('GPU', fontsize=12)
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('moe_all2all_visualization.png', dpi=300, bbox_inches='tight')
    print("âœ… å¯è§†åŒ–å›¾å·²ä¿å­˜ä¸º 'moe_all2all_visualization.png'")
    plt.show()

def analyze_communication_patterns():
    """åˆ†æé€šä¿¡æ¨¡å¼"""
    print("\nğŸ” é€šä¿¡æ¨¡å¼åˆ†æ:")
    print("=" * 60)
    
    # å¸¦å®½åˆ†æ
    nvlink_bandwidth = 400.0  # GB/s
    ib_bandwidth = 50.0  # GB/s
    
    print(f"ğŸ“¡ å¸¦å®½é…ç½®:")
    print(f"   - NVLinkå¸¦å®½: {nvlink_bandwidth} GB/s")
    print(f"   - InfiniBandå¸¦å®½: {ib_bandwidth} GB/s")
    
    # Prefillé˜¶æ®µé€šä¿¡åˆ†æ
    prefill_send_volume = 0.5  # å‡è®¾å€¼ï¼Œå®é™…ä»ä¸Šé¢è®¡ç®—
    prefill_intra_node_comm = prefill_send_volume * 7/8  # èŠ‚ç‚¹å†…é€šä¿¡
    prefill_inter_node_comm = prefill_send_volume * 1/8  # èŠ‚ç‚¹é—´é€šä¿¡
    
    prefill_intra_time = prefill_intra_node_comm / nvlink_bandwidth
    prefill_inter_time = prefill_inter_node_comm / ib_bandwidth
    prefill_total_time = max(prefill_intra_time, prefill_inter_time)
    
    print(f"\nğŸ“Š Prefillé€šä¿¡æ—¶é—´åˆ†æ:")
    print(f"   - èŠ‚ç‚¹å†…é€šä¿¡: {prefill_intra_node_comm:.3f} GB")
    print(f"   - èŠ‚ç‚¹é—´é€šä¿¡: {prefill_inter_node_comm:.3f} GB")
    print(f"   - èŠ‚ç‚¹å†…æ—¶é—´: {prefill_intra_time*1000:.2f} ms")
    print(f"   - èŠ‚ç‚¹é—´æ—¶é—´: {prefill_inter_time*1000:.2f} ms")
    print(f"   - æ€»é€šä¿¡æ—¶é—´: {prefill_total_time*1000:.2f} ms")
    
    # Decodeé˜¶æ®µé€šä¿¡åˆ†æ
    decode_send_volume = 0.001  # å‡è®¾å€¼
    decode_intra_node_comm = decode_send_volume * 7/8
    decode_inter_node_comm = decode_send_volume * 1/8
    
    decode_intra_time = decode_intra_node_comm / nvlink_bandwidth
    decode_inter_time = decode_inter_node_comm / ib_bandwidth
    decode_total_time = max(decode_intra_time, decode_inter_time)
    
    print(f"\nğŸ“Š Decodeé€šä¿¡æ—¶é—´åˆ†æ:")
    print(f"   - èŠ‚ç‚¹å†…é€šä¿¡: {decode_intra_node_comm:.6f} GB")
    print(f"   - èŠ‚ç‚¹é—´é€šä¿¡: {decode_inter_node_comm:.6f} GB")
    print(f"   - èŠ‚ç‚¹å†…æ—¶é—´: {decode_intra_time*1000:.4f} ms")
    print(f"   - èŠ‚ç‚¹é—´æ—¶é—´: {decode_inter_time*1000:.4f} ms")
    print(f"   - æ€»é€šä¿¡æ—¶é—´: {decode_total_time*1000:.4f} ms")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ DeepSeek V3 MoE é€šä¿¡é‡åˆ†æä¸å¯è§†åŒ–")
    print("=" * 80)
    
    # åˆ†æé€šä¿¡é‡
    results = analyze_moe_communication()
    
    # åˆ†æé€šä¿¡æ¨¡å¼
    analyze_communication_patterns()
    
    # å¯è§†åŒ–
    visualize_all2all_process()
    
    print("\nâœ… åˆ†æå®Œæˆ!")

if __name__ == "__main__":
    main() 